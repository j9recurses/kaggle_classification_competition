{
 "metadata": {
  "name": "",
  "signature": "sha256:4cfc9d02ea3e72b22660733e89b7bf1b923cd18fff8d37ce967df4fe667ae099"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk \n",
      "import random\n",
      "from random import shuffle \n",
      "from collections import Counter\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "import nltk.tag, nltk.data\n",
      "from nltk.corpus import wordnet\n",
      "from nltk.stem.porter import PorterStemmer\n",
      "import numpy as np\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn import metrics\n",
      "from operator import itemgetter\n",
      "from sklearn.metrics import classification_report\n",
      "import csv\n",
      "import os\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 265
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Class to pre-process the collection\n",
      "#class is full of goodies/functions for preprocessing data- remove stopwords, punk, and convert to lower, lemmatize \n",
      "class PreprocessText:\n",
      "\n",
      "    #function to remove punct. \n",
      "    def remove_punct(self, text):\n",
      "      exclude = set(string.punctuation)\n",
      "      table = string.maketrans(\"\",\"\")\n",
      "      text = text.translate(table, string.punctuation)\n",
      "      return text\n",
      "\n",
      "    #remove stopwords-> A quick way to reduce elminate words that aren't valid key words.\n",
      "    def removestopwords(self, tokens):\n",
      "      stopwords = nltk.corpus.stopwords.words('english')\n",
      "      tokens = [w for w in tokens if w.lower().strip() not in stopwords]\n",
      "      return tokens\n",
      "\n",
      "    ##lemmatize the words to reduce dimensionality. Also,option to do lemmatization based on POS. \n",
      "    #wordnet lemmatizer assumes everything is a noun unless otherwise specified, so we need to give\n",
      "    #it the wordnet pos if we don't want the default noun lookup. \n",
      "    def lemmatize(self, tokens, lemmatize_pos):\n",
      "        def get_wordnet_pos( pos_tag):\n",
      "            if pos_tag[1].startswith('J'):\n",
      "                return (pos_tag[0], wordnet.ADJ)\n",
      "            elif pos_tag[1].startswith('V'):\n",
      "                return (pos_tag[0], wordnet.VERB)\n",
      "            elif pos_tag[1].startswith('N'):\n",
      "                return (pos_tag[0], wordnet.NOUN)\n",
      "            elif pos_tag[1].startswith('R'):\n",
      "                return (pos_tag[0], wordnet.ADV)\n",
      "            else:\n",
      "                return (pos_tag[0], wordnet.NOUN)\n",
      "        lemmatizer = WordNetLemmatizer()\n",
      "        if lemmatize_pos:\n",
      "            tokens_pos = nltk.tag.pos_tag(tokens)\n",
      "            tokens_pos_wordnet = [ get_wordnet_pos(token) for token in tokens_pos]\n",
      "            tokens_lemm = [lemmatizer.lemmatize(token[0], token[1]) for token in tokens_pos_wordnet]\n",
      "        else:\n",
      "            tokens_lemm = [lemmatizer.lemmatize(token) for token in tokens] \n",
      "        return tokens_lemm\n",
      "    \n",
      "    #function that combines above functions in one routine\n",
      "    #lots of args to specify what preprocessing routine you want to use\n",
      "    def preprocess_txt(self, text, convertlower=True, nopunk=True, stopwords=True, lemmatize_doc=True, lemmatize_pos=True, stemmed=False):\n",
      "      #convert to lower\n",
      "      if convertlower:\n",
      "        text = text.lower()\n",
      "      # remove punctuation\n",
      "      if nopunk:\n",
      "        text = remove_punct(text)\n",
      "      #tokenize text\n",
      "      tokens = PunktWordTokenizer().tokenize(text)\n",
      "      #remove stopwords\n",
      "      if stopwords:\n",
      "        tokens = removestopwords(tokens)\n",
      "      #lemmatize\n",
      "      if lemmatize_doc:\n",
      "        tokens = self.lemmatize(tokens,lemmatize_pos)\n",
      "      #stem\n",
      "        porter = PorterStemmer()\n",
      "        tokens = [ porter.stem(token) for token in tokens ]\n",
      "      return tokens\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 117
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#function to break up the data into training \n",
      "def prep_data():\n",
      "    f = open('data/train.txt','rb')\n",
      "    train_raw = f.read()\n",
      "    f.close() \n",
      "    train_split = train_raw.split('\\n')\n",
      "    train_tuples = [ (line[2:], line[0]) for line in train_split if line != '']\n",
      "    shuffle(train_tuples)\n",
      "    total_size = len(train_tuples)\n",
      "    train_size = int(total_size * 0.9) \n",
      "    return train_tuples[:train_size], train_tuples[train_size:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 209
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get the test and training sets, print the size\n",
      "yahoo_train, yahoo_test = prep_data()\n",
      "print 'The training set size is ' + str(len(yahoo_train))\n",
      "print 'The test set size is ' + str(len(yahoo_test))  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The training set size is 2428\n",
        "The test set size is 270\n"
       ]
      }
     ],
     "prompt_number": 210
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#pt = PreprocessText()\n",
      "#yahoo_train_pr = [( sent[1], pt.preprocess_txt(sent[0])) for sent in yahoo_train ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 133
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#transform the training set into a dict, get the keys and vals for the tet and train\n",
      "def get_train_and_test_dicts(yahoo_train, yahoo_test):\n",
      "    yahoo_train_keys = dict(yahoo_train).keys()\n",
      "    #yahoo_train_keys = [unicode(word) for word in yahoo_train_keys]\n",
      "    yahoo_train_vals = dict(yahoo_train).values()\n",
      "    yahoo_test_keys = dict(yahoo_test).keys()\n",
      "    #yahoo_test_keys = [unicode(word) for word in yahoo_test_keys ]\n",
      "    yahoo_test_vals = dict(yahoo_test).values()\n",
      "    return np.array(yahoo_train_keys), np.array(yahoo_train_vals), np.array(yahoo_test_keys), np.array(yahoo_test_vals)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 211
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_words, train_cats, test_words, test_cats = get_train_and_test_dicts(yahoo_train, yahoo_test)\n",
      "#lol. Don't need to do preprocessing- Scipylearn can do all the preprocessing for you\n",
      "#want to use tfidf vectors to just grab the words that are most relevant\n",
      "#Convert a collection of raw documents to a matrix of TF-IDF features.\n",
      "#http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
      "\n",
      "#Convert a collection of raw documents to a matrix of TF-IDF features\n",
      "vectorizer = TfidfVectorizer(ngram_range=(1, 1), stop_words='english', strip_accents='unicode')\n",
      "\n",
      "#see what scipy is doing:\n",
      "test_string = unicode(yahoo_train_words[0])\n",
      "\n",
      "print \"Example string: \" + test_string\n",
      "print \"Preprocessed string: \" + vectorizer.build_preprocessor()(test_string)\n",
      "print \"Tokenized string:\" + str(vectorizer.build_tokenizer()(test_string))\n",
      "print \"N-gram data string:\" + str(vectorizer.build_analyzer()(test_string))\n",
      "print \"\\n\"\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Example string: in reference to the saying what goes up must come down; as the universe expands must'nt it also collapse? \n",
        "Preprocessed string: in reference to the saying what goes up must come down; as the universe expands must'nt it also collapse? \n",
        "Tokenized string:[u'in', u'reference', u'to', u'the', u'saying', u'what', u'goes', u'up', u'must', u'come', u'down', u'as', u'the', u'universe', u'expands', u'must', u'nt', u'it', u'also', u'collapse']\n",
        "N-gram data string:[u'reference', u'saying', u'goes', u'come', u'universe', u'expands', u'nt', u'collapse']\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 239
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Created a vocabulary from the training data (the fit in .fit_transform())\n",
      "#then turn the words into a TF-IDF weighted word vector(the transform in .fit_transform())\n",
      "\n",
      "X_train_words = vectorizer.fit_transform(yahoo_train_words)\n",
      "\n",
      "#transformed the test data into a TF-IDF weighted word vector in the vocab space of the training data(.transform())\n",
      "X_test_words = vectorizer.transform(yahoo_test_words)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 253
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#build the classifier\n",
      "bayes_classifier = MultinomialNB().fit(X_train_words, train_cats)\n",
      "\n",
      "yahoo_predicted = nb_classifier.predict(X_test_words)\n",
      "\n",
      "print \"MODEL: Multinomial Naive Bayes\"\n",
      "\n",
      "print 'The precision for this classifier is ' + str(metrics.precision_score(test_cats, yahoo_predicted))\n",
      "print 'The recall for this classifier is ' + str(metrics.recall_score(test_cats, yahoo_predicted))\n",
      "print 'The f1 for this classifier is ' + str(metrics.f1_score(test_cats, yahoo_predicted))\n",
      "print 'The accuracy for this classifier is ' + str(metrics.accuracy_score(test_cats, yahoo_predicted))\n",
      "\n",
      "print '\\nHere is the classification report:'\n",
      "print classification_report(test_cats, yahoo_predicted)\n",
      "\n",
      "#simple thing to do would be to up the n-grams to bigrams; try varying ngram_range from (1, 1) to (1, 2)\n",
      "#we could also modify the vectorizer to stem or lemmatize\n",
      "print '\\nHere is the confusion matrix:'\n",
      "print metrics.confusion_matrix(test_cats, yahoo_predicted)\n",
      "\n",
      "##print the \n",
      "N = 10\n",
      "vocabulary = np.array([t for t, i in sorted(vectorizer.vocabulary_.iteritems(), key=itemgetter(1))])\n",
      "\n",
      "for i, label in enumerate(set(test_cats)):\n",
      " topN = np.argsort(bayes_classifier.coef_[i])[-N:]\n",
      " print \"\\nThe top %d most informative features for topic code %s: \\n%s\" % (N, label, \" \".join(vocabulary[topN]))\n",
      " #print topN\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MODEL: Multinomial Naive Bayes\n",
        "The precision for this classifier is 0.460883489774\n",
        "The recall for this classifier is 0.411111111111\n",
        "The f1 for this classifier is 0.354065020596\n",
        "The accuracy for this classifier is 0.411111111111\n",
        "\n",
        "Here is the classification report:\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          1       0.31      0.84      0.45        74\n",
        "          2       0.66      0.57      0.61        44\n",
        "          3       0.85      0.23      0.37        47\n",
        "          4       0.67      0.32      0.44        37\n",
        "          5       0.00      0.00      0.00        23\n",
        "          6       0.50      0.06      0.11        16\n",
        "          7       0.00      0.00      0.00        29\n",
        "\n",
        "avg / total       0.46      0.41      0.35       270\n",
        "\n",
        "\n",
        "Here is the confusion matrix:\n",
        "[[62  7  1  4  0  0  0]\n",
        " [18 25  0  1  0  0  0]\n",
        " [32  4 11  0  0  0  0]\n",
        " [25  0  0 12  0  0  0]\n",
        " [21  0  1  0  0  1  0]\n",
        " [14  0  0  1  0  1  0]\n",
        " [27  2  0  0  0  0  0]]\n",
        "\n",
        "The top 10 most informative features for topic code 1: \n",
        "make xa money like want does know people yahoo best\n",
        "\n",
        "The top 10 most informative features for topic code 3: \n",
        "does free internet windows web best use xa computer yahoo\n",
        "\n",
        "The top 10 most informative features for topic code 2: \n",
        "does think tv did best music favorite movie song xa\n",
        "\n",
        "The top 10 most informative features for topic code 5: \n",
        "women want relationship boyfriend guy know friend like girl love\n",
        "\n",
        "The top 10 most informative features for topic code 4: \n",
        "study help need language good words does college word school\n",
        "\n",
        "The top 10 most informative features for topic code 7: \n",
        "fight smoking rid cold know help pain way does best\n",
        "\n",
        "The top 10 most informative features for topic code 6: \n",
        "moon time universe possible gas planet earth xa life does\n"
       ]
      }
     ],
     "prompt_number": 267
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##now try another classifier\n",
      "\n",
      "svm_classifier = LinearSVC().fit(X_train_words, train_cats)\n",
      "\n",
      "yahoo_svm_predicted = svm_classifier.predict(X_test_words)\n",
      "print \"MODEL: Linear SVC\"\n",
      "\n",
      "print 'The precision for this classifier is ' + str(metrics.precision_score(test_cats, yahoo_svm_predicted))\n",
      "print 'The recall for this classifier is ' + str(metrics.recall_score(test_cats, yahoo_svm_predicted))\n",
      "print 'The f1 for this classifier is ' + str(metrics.f1_score(test_cats, yahoo_svm_predicted))\n",
      "print 'The accuracy for this classifier is ' + str(metrics.accuracy_score(test_cats, yahoo_svm_predicted))\n",
      "\n",
      "print '\\nHere is the classification report:'\n",
      "print classification_report(test_cats, yahoo_svm_predicted)\n",
      "\n",
      "#simple thing to do would be to up the n-grams to bigrams; try varying ngram_range from (1, 1) to (1, 2)\n",
      "#we could also modify the vectorizer to stem or lemmatize\n",
      "print '\\nHere is the confusion matrix:'\n",
      "print metrics.confusion_matrix(test_cats, yahoo_svm_predicted)\n",
      "\n",
      "N = 10\n",
      "vocabulary = np.array([t for t, i in sorted(vectorizer.vocabulary_.iteritems(), key=itemgetter(1))])\n",
      "\n",
      "for i, label in enumerate(set(test_cats)):\n",
      " topN = np.argsort(svm_classifier.coef_[i])[-N:]\n",
      " print \"\\nThe top %d most informative features for topic code %s: \\n%s\" % (N, label, \" \".join(vocabulary[topN]))\n",
      " #print topN\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MODEL: Linear SVC\n",
        "The precision for this classifier is 0.530691487417\n",
        "The recall for this classifier is 0.52962962963\n",
        "The f1 for this classifier is 0.52508172486\n",
        "The accuracy for this classifier is 0.52962962963\n",
        "\n",
        "Here is the classification report:\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          1       0.39      0.47      0.43        74\n",
        "          2       0.65      0.70      0.67        44\n",
        "          3       0.64      0.60      0.62        47\n",
        "          4       0.69      0.68      0.68        37\n",
        "          5       0.43      0.26      0.32        23\n",
        "          6       0.53      0.62      0.57        16\n",
        "          7       0.42      0.28      0.33        29\n",
        "\n",
        "avg / total       0.53      0.53      0.53       270\n",
        "\n",
        "\n",
        "Here is the confusion matrix:\n",
        "[[35 12  5  7  3  3  9]\n",
        " [10 31  1  1  0  0  1]\n",
        " [ 8  3 28  1  3  3  1]\n",
        " [12  0  0 25  0  0  0]\n",
        " [ 9  1  4  0  6  3  0]\n",
        " [ 4  0  0  2  0 10  0]\n",
        " [12  1  6  0  2  0  8]]\n",
        "\n",
        "The top 10 most informative features for topic code 1: \n",
        "santa money bank restaurant addicted public things stock credit sending\n",
        "\n",
        "The top 10 most informative features for topic code 3: \n",
        "programming mac password linux rss java windows internet use computer\n",
        "\n",
        "The top 10 most informative features for topic code 2: \n",
        "celebrities actress film movies magazine rock tv movie music song\n",
        "\n",
        "The top 10 most informative features for topic code 5: \n",
        "dating married husband family ex date friend love marriage relationship\n",
        "\n",
        "The top 10 most informative features for topic code 4: \n",
        "mba words word college schools shot education study colleges school\n",
        "\n",
        "The top 10 most informative features for topic code 7: \n",
        "pain teeth cure smoking body fight weight aids surgery diet\n",
        "\n",
        "The top 10 most informative features for topic code 6: \n",
        "humans universe earth electricity nipples eyes stars planet theory math\n"
       ]
      }
     ],
     "prompt_number": 268
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "###now try max ent as a classifier\n",
      "maxent_classifier = LogisticRegression().fit(X_train_words, train_cats)\n",
      "\n",
      "yahoo_maxent_predicted = maxent_classifier.predict(X_test_words)\n",
      "print \"MODEL: Maximum Entropy\"\n",
      "\n",
      "print 'The precision for this classifier is ' + str(metrics.precision_score(test_cats, yahoo_maxent_predicted))\n",
      "print 'The recall for this classifier is ' + str(metrics.recall_score(test_cats, yahoo_maxent_predicted))\n",
      "print 'The f1 for this classifier is ' + str(metrics.f1_score(test_cats, yahoo_maxent_predicted))\n",
      "print 'The accuracy for this classifier is ' + str(metrics.accuracy_score(test_cats, yahoo_maxent_predicted))\n",
      "\n",
      "print '\\nHere is the classification report:'\n",
      "print classification_report(test_cats, yahoo_maxent_predicted)\n",
      "\n",
      "#simple thing to do would be to up the n-grams to bigrams; try varying ngram_range from (1, 1) to (1, 2)\n",
      "#we could also modify the vectorizer to stem or lemmatize\n",
      "print '\\nHere is the confusion matrix:'\n",
      "print metrics.confusion_matrix(test_cats, yahoo_maxent_predicted)\n",
      "\n",
      "N = 10\n",
      "vocabulary = np.array([t for t, i in sorted(vectorizer.vocabulary_.iteritems(), key=itemgetter(1))])\n",
      "\n",
      "for i, label in enumerate(set(test_cats)):\n",
      " topN = np.argsort(maxent_classifier.coef_[i])[-N:]\n",
      " print \"\\nThe top %d most informative features for topic code %s: \\n%s\" % (N, label, \" \".join(vocabulary[topN]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MODEL: Maximum Entropy\n",
        "The precision for this classifier is 0.601634071921\n",
        "The recall for this classifier is 0.507407407407\n",
        "The f1 for this classifier is 0.473196291899\n",
        "The accuracy for this classifier is 0.507407407407\n",
        "\n",
        "Here is the classification report:\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          1       0.36      0.80      0.50        74\n",
        "          2       0.68      0.64      0.66        44\n",
        "          3       0.82      0.49      0.61        47\n",
        "          4       0.67      0.54      0.60        37\n",
        "          5       0.00      0.00      0.00        23\n",
        "          6       0.83      0.31      0.45        16\n",
        "          7       1.00      0.07      0.13        29\n",
        "\n",
        "avg / total       0.60      0.51      0.47       270\n",
        "\n",
        "\n",
        "Here is the confusion matrix:\n",
        "[[59  7  2  6  0  0  0]\n",
        " [15 28  0  1  0  0  0]\n",
        " [21  3 23  0  0  0  0]\n",
        " [16  1  0 20  0  0  0]\n",
        " [19  1  2  0  0  1  0]\n",
        " [ 8  0  0  3  0  5  0]\n",
        " [25  1  1  0  0  0  2]]\n",
        "\n",
        "The top 10 most informative features for topic code 1: \n",
        "santa home house stock people buy business job credit money\n",
        "\n",
        "The top 10 most informative features for topic code 3: \n",
        "page website linux web software internet yahoo windows use computer\n",
        "\n",
        "The top 10 most informative features for topic code 2: \n",
        "singer did magazine movies rock favorite tv music movie song\n",
        "\n",
        "The top 10 most informative features for topic code 5: \n",
        "girls women date guy marriage boyfriend girl friend relationship love\n",
        "\n",
        "The top 10 most informative features for topic code 4: \n",
        "degree language colleges schools education study words word college school\n",
        "\n",
        "The top 10 most informative features for topic code 7: \n",
        "body fight blood causes aids rid smoking surgery diet pain\n",
        "\n",
        "The top 10 most informative features for topic code 6: \n",
        "humans moon universe gas theory stars life math planet earth\n"
       ]
      }
     ],
     "prompt_number": 269
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#function to throw out the most occuring and least occuring words;\n",
      "#do this by term frequency for a word, and then figuring out the overall tf distribution \n",
      "#of words in the collection\n",
      "def get_ngram_cutoff(ngram, lowerbound, upperbound):\n",
      "  fdist_ngram = nltk.FreqDist(ngram)\n",
      "  fdist_keys = fdist_ngram.keys()\n",
      "  fdist_vals =  fdist_ngram.values()\n",
      "  percentilescore = [stats.percentileofscore(fdist_vals, i) for i in fdist_vals]\n",
      "  new_fdist = dict(zip(fdist_keys, percentilescore))\n",
      "  #get the keywords:\n",
      "  keywords = []  \n",
      "  new_key_word_dict = []\n",
      "  #if the term freq of a word is between the xth and yth percentile, keep it; if not, throw it out. \n",
      "  for word, percentile in new_fdist.iteritems():\n",
      "      if percentile > lowerbound and percentile < upperbound:\n",
      "         #get the words freq count\n",
      "        #new_key_word_dict[word[0]] = fdist_ngram[word]\n",
      "        new_key_word_dict.append( (word) + (fdist_ngram[word],))\n",
      "  sorted_keywords = sorted_key_word_dict = sorted(new_key_word_dict,  key=lambda tup: tup[2], reverse = True)\n",
      "  return sorted_keywords "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 244
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##might be worth it to run a spell checker:\n",
      "import re, collections\n",
      "\n",
      "def words(text): return re.findall('[a-z]+', text.lower()) \n",
      "\n",
      "def train(features):\n",
      "    model = collections.defaultdict(lambda: 1)\n",
      "    for f in features:\n",
      "        model[f] += 1\n",
      "    return model\n",
      "\n",
      "NWORDS = train(words(file('big.txt').read()))\n",
      "\n",
      "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
      "\n",
      "def edits1(word):\n",
      "   splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
      "   deletes    = [a + b[1:] for a, b in splits if b]\n",
      "   transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]\n",
      "   replaces   = [a + c + b[1:] for a, b in splits for c in alphabet if b]\n",
      "   inserts    = [a + c + b     for a, b in splits for c in alphabet]\n",
      "   return set(deletes + transposes + replaces + inserts)\n",
      "\n",
      "def known_edits2(word):\n",
      "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in NWORDS)\n",
      "\n",
      "def known(words): return set(w for w in words if w in NWORDS)\n",
      "\n",
      "def correct(word):\n",
      "    candidates = known([word]) or known(edits1(word)) or known_edits2(word) or [word]\n",
      "    return max(candidates, key=NWORDS.get)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}