{
 "metadata": {
  "name": "",
  "signature": "sha256:996d3310aa44dbf78528b38364f76cc9f80f5245b4fec462e1a1ca5fe1be12a9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk \n",
      "import random\n",
      "from random import shuffle \n",
      "from collections import Counter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def prep_data():\n",
      "    f = open('/home/j9/Desktop/fall_2014/256nlp/kaggle_classification_competition/data/train.txt','r')\n",
      "    train_raw = f.read()\n",
      "    f.close() \n",
      "    train_split = train_raw.split('\\n')\n",
      "    train_tuples = [ (line[2:], line[0]) for line in train_split if line != '']\n",
      "    \n",
      "    \n",
      "    shuffle(train_tuples)\n",
      "    total_size = len(train_tuples)\n",
      "    train_size = int(total_size * 0.9) \n",
      "    return train_tuples[:train_size], train_tuples[train_size:]\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "yahoo_train, yahoo_test = prep_data()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#function to throw out the most occuring and least occuring words;\n",
      "#do this by term frequency for a word, and then figuring out the overall tf distribution \n",
      "#of words in the collection\n",
      "def get_ngram_cutoff(ngram, lowerbound, upperbound):\n",
      "  fdist_ngram = nltk.FreqDist(ngram)\n",
      "  fdist_keys = fdist_ngram.keys()\n",
      "  fdist_vals =  fdist_ngram.values()\n",
      "  percentilescore = [stats.percentileofscore(fdist_vals, i) for i in fdist_vals]\n",
      "  new_fdist = dict(zip(fdist_keys, percentilescore))\n",
      "  #get the keywords:\n",
      "  keywords = []  \n",
      "  new_key_word_dict = []\n",
      "  #if the term freq of a word is between the xth and yth percentile, keep it; if not, throw it out. \n",
      "  for word, percentile in new_fdist.iteritems():\n",
      "      if percentile > lowerbound and percentile < upperbound:\n",
      "         #get the words freq count\n",
      "        #new_key_word_dict[word[0]] = fdist_ngram[word]\n",
      "        new_key_word_dict.append( (word) + (fdist_ngram[word],))\n",
      "  sorted_keywords = sorted_key_word_dict = sorted(new_key_word_dict,  key=lambda tup: tup[2], reverse = True)\n",
      "  return sorted_keywords "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Using the POS data we now have from the unigramming we just did, we now have the ability to \n",
      "#accurately try to lemmatize our collection. \n",
      "# lemmatizing can further reduce the potential keys words by returning word variations into their normative form. \n",
      "lemmatizer = WordNetLemmatizer()\n",
      "\n",
      "def get_wordnet_pos(pos_tag):\n",
      "    if pos_tag[1].startswith('J'):\n",
      "        return (pos_tag[0], wordnet.ADJ)\n",
      "    elif pos_tag[1].startswith('V'):\n",
      "        return (pos_tag[0], wordnet.VERB)\n",
      "    elif pos_tag[1].startswith('N'):\n",
      "        return (pos_tag[0], wordnet.NOUN)\n",
      "    elif pos_tag[1].startswith('R'):\n",
      "        return (pos_tag[0], wordnet.ADV)\n",
      "    else:\n",
      "        return (pos_tag[0], wordnet.NOUN)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#to use the lemmatizer: tag the words with POS tagger Then run through like this: \n",
      "mystery_tokens_uni_pos_wordnet = [get_wordnet_pos(item) for item in mystery_unigrams]\n",
      "mystery_lemm_unigrams = [ (lemmatizer.lemmatize(item.strip(string.punctuation), pos),) +(pos,) for item, pos in mystery_tokens_uni_pos_wordnet]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}