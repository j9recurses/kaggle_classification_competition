{
 "metadata": {
  "name": "",
  "signature": "sha256:a4a80c86fd8838c6eafce8f1bc4bcde741f7b8e6a65da8c343aef7290d132e1a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk \n",
      "from nltk.corpus import gutenberg\n",
      "import random\n",
      "from random import shuffle \n",
      "from collections import Counter\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "from nltk.tokenize.punkt import PunktWordTokenizer\n",
      "import nltk.tag, nltk.data\n",
      "from nltk.corpus import wordnet\n",
      "from nltk.stem.porter import PorterStemmer\n",
      "import numpy as np\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn import metrics\n",
      "from operator import itemgetter\n",
      "from sklearn.metrics import classification_report\n",
      "import csv\n",
      "import os\n",
      "import collections\n",
      "import operator\n",
      "from collections import OrderedDict\n",
      "import string\n",
      "import re\n",
      "from nltk.corpus import reuters\n",
      "from nltk.corpus import brown"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 501
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#function to break up the data into training \n",
      "def prep_data():\n",
      "    f = open('data/train.txt','rb')\n",
      "    train_raw = f.read()\n",
      "    f.close() \n",
      "    train_split = train_raw.split('\\n')\n",
      "    train_tuples = [ ( line[2:], line[0]) for line in train_split if line != '']\n",
      "    shuffle(train_tuples)\n",
      "    total_size = len(train_tuples)\n",
      "    train_size = int(total_size * 0.9) \n",
      "    return train_tuples[:train_size], train_tuples[train_size:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 502
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get the test and training sets, print the size\n",
      "yahoo_train, yahoo_test = prep_data()\n",
      "print 'The training set size is ' + str(len(yahoo_train))\n",
      "print 'The test set size is ' + str(len(yahoo_test)) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The training set size is 2428\n",
        "The test set size is 270\n"
       ]
      }
     ],
     "prompt_number": 503
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#transform the training set into a dict, get the keys and vals for the tet and train\n",
      "def get_train_and_test_dicts(yahoo_train_dict, yahoo_test_dict):\n",
      "    yahoo_train_keys = yahoo_train_dict.keys()\n",
      "    #yahoo_train_keys = [unicode(word) for word in yahoo_train_keys]\n",
      "    yahoo_train_vals = yahoo_train_dict.values()\n",
      "    yahoo_test_keys = yahoo_test_dict.keys()\n",
      "    #yahoo_test_keys = [unicode(word) for word in yahoo_test_keys ]\n",
      "    yahoo_test_vals = yahoo_test_dict.values()\n",
      "    return np.array(yahoo_train_keys), np.array(yahoo_train_vals), np.array(yahoo_test_keys), np.array(yahoo_test_vals)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 504
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Created a vocabulary from the training data (the fit in .fit_transform())\n",
      "#then turn the words into a TF-IDF weighted word vector(the transform in .fit_transform())\n",
      "#Convert a collection of raw documents to a matrix of TF-IDF features\n",
      "\n",
      "#lol. Don't necessarily need to do preprocessing- Scipylearn can do all the preprocessing for you\n",
      "#want to use tfidf vectors to just grab the words that are most relevant\n",
      "#Convert a collection of raw documents to a matrix of TF-IDF features.\n",
      "#http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
      "\n",
      "vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english', strip_accents='unicode')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 505
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#split the test and train into numpy arrays so they can be vectorized and trained with\n",
      "yahoo_train_dict  =  dict(yahoo_train)\n",
      "yahoo_test_dict  =  dict(yahoo_test)\n",
      "train_words, train_cats,  test_words,  test_cats = get_train_and_test_dicts(yahoo_train_dict, yahoo_test_dict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 506
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print train_cats[1]\n",
      "print train_words[1]\n",
      "print test_cats[1]\n",
      "print test_words[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "7\n",
        "in reference to the saying what goes up must come down; as the universe expands must'nt it also collapse? \n",
        "2\n",
        "why does yahoo limit the number of answers you can give on yahoo! answers? \n"
       ]
      }
     ],
     "prompt_number": 507
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#example of how the vectorizer works\n",
      "test_string = unicode(train_words[0])\n",
      "print \"Example string: \" + test_string\n",
      "print \"Preprocessed string: \" + vectorizer.build_preprocessor()(test_string)\n",
      "print \"Tokenized string:\" + str(vectorizer.build_tokenizer()(test_string))\n",
      "print \"N-gram data string:\" + str(vectorizer.build_analyzer()(test_string))\n",
      "print \"\\n\"\n",
      "\n",
      "#Created a vocabulary from the training data (the fit in .fit_transform())\",\n",
      "#then turn the words into a TF-IDF weighted word vector(the transform in .fit_transform())\"\n",
      "X_train_words = vectorizer.fit_transform(train_words)\n",
      "\n",
      "#transformed the test data into a TF-IDF weighted word vector in the vocab space of the training data(.transform())\n",
      "X_test_words = vectorizer.transform(test_words)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Example string: is the sun really yellow? i wanted to know if the sun was really yellow or was that a far away look at it. is it red or orange what color is it.\n",
        "Preprocessed string: is the sun really yellow? i wanted to know if the sun was really yellow or was that a far away look at it. is it red or orange what color is it.\n",
        "Tokenized string:[u'is', u'the', u'sun', u'really', u'yellow', u'wanted', u'to', u'know', u'if', u'the', u'sun', u'was', u'really', u'yellow', u'or', u'was', u'that', u'far', u'away', u'look', u'at', u'it', u'is', u'it', u'red', u'or', u'orange', u'what', u'color', u'is', u'it']\n",
        "N-gram data string:[u'sun', u'really', u'yellow', u'wanted', u'know', u'sun', u'really', u'yellow', u'far', u'away', u'look', u'red', u'orange', u'color', u'sun really', u'really yellow', u'yellow wanted', u'wanted know', u'know sun', u'sun really', u'really yellow', u'yellow far', u'far away', u'away look', u'look red', u'red orange', u'orange color']\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 508
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#function to evaluate results of each model\n",
      "# code parts inspired from this presentation: http://stanford.edu/~rjweiss/public_html/IRiSS2013/text2/#1\n",
      "def evaluate_results(model_name, test_cats, predicted, myclassifier, myvectorizer, prwords = True):\n",
      "        print model_name\n",
      "        print 'The precision for this classifier is ' + str(metrics.precision_score(test_cats, predicted))\n",
      "        print 'The recall for this classifier is ' + str(metrics.recall_score(test_cats, predicted))\n",
      "        print 'The f1 for this classifier is ' + str(metrics.f1_score(test_cats, predicted))\n",
      "        print 'The accuracy for this classifier is ' + str(metrics.accuracy_score(test_cats, predicted))\n",
      "        print '\\nHere is the classification report:'\n",
      "        print classification_report(test_cats, predicted)\n",
      "        print '\\nHere is the confusion matrix:'\n",
      "        print metrics.confusion_matrix(test_cats, predicted)\n",
      "        ##print the top 10 words for each category\n",
      "        if prwords == True:\n",
      "            N = 10\n",
      "            vocabulary = np.array([t for t, i in sorted(myvectorizer.vocabulary_.iteritems(), key=itemgetter(1))])\n",
      "            for i, label in enumerate(set(test_cats)):\n",
      "                topN = np.argsort(myclassifier.coef_[i])[-N:]\n",
      "                print \"\\nThe top %d most informative features for topic code %s: \\n%s\" % (N, label, \" \".join(vocabulary[topN]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 509
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##bayes classifier\n",
      "def get_bayes(X_train_words, train_cats, X_test_words):\n",
      "    #build the classifier\n",
      "    bayes_classifier = MultinomialNB().fit(X_train_words,train_cats )\n",
      "    yahoo_bayes_predicted = bayes_classifier.predict(X_test_words)\n",
      "    return bayes_classifier, yahoo_bayes_predicted"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 510
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#SVM classifier\n",
      "def get_svm(X_train_words, train_cats, X_test_words):\n",
      "    svm_classifier = LinearSVC().fit(X_train_words, train_cats)\n",
      "    yahoo_svm_predicted = svm_classifier.predict(X_test_words)\n",
      "    return svm_classifier, yahoo_svm_predicted"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 511
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "###now try max ent as a classifier\n",
      "def get_maxent(X_train_words, train_cats, X_test_words):\n",
      "    maxent_classifier = LogisticRegression().fit(X_train_words, train_cats)\n",
      "    yahoo_maxent_predicted = maxent_classifier.predict(X_test_words)\n",
      "    return maxent_classifier, yahoo_maxent_predicted"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 512
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get the bayes resuts \n",
      "bayes_classifier, yahoo_bayes_predicted = get_bayes(X_train_words, train_cats, X_test_words )\n",
      "bayes_model_name =  \"MODEL: Multinomial Naive Bayes\"\n",
      "evaluate_results(bayes_model_name, test_cats, yahoo_bayes_predicted, bayes_classifier, vectorizer)\n",
      "#then get the SVM results:\n",
      "print \"*****\"\n",
      "svm_classifier, yahoo_svm_predicted = get_svm(X_train_words, train_cats, X_test_words )\n",
      "svm_model_name = \"MODEL: Linear SVC\"\n",
      "evaluate_results(svm_model_name, test_cats, yahoo_svm_predicted, svm_classifier, vectorizer)\n",
      "print \"*****\"\n",
      "#then max ent results\n",
      "maxent_classifier, yahoo_maxent_predicted = get_maxent(X_train_words, train_cats, X_test_words )\n",
      "maxent_model_name = \"MODEL: Maximum Entropy\"\n",
      "evaluate_results(maxent_model_name, test_cats, yahoo_maxent_predicted, maxent_classifier, vectorizer)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MODEL: Multinomial Naive Bayes\n",
        "The precision for this classifier is 0.567333603166\n",
        "The recall for this classifier is 0.481481481481\n",
        "The f1 for this classifier is 0.412563941626\n",
        "The accuracy for this classifier is 0.481481481481\n",
        "\n",
        "Here is the classification report:\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          1       0.39      0.93      0.55        89\n",
        "          2       0.79      0.61      0.69        38\n",
        "          3       0.86      0.26      0.40        46\n",
        "          4       0.85      0.32      0.47        34\n",
        "          5       0.00      0.00      0.00        21\n",
        "          6       1.00      0.05      0.10        20\n",
        "          7       0.00      0.00      0.00        22\n",
        "\n",
        "avg / total       0.57      0.48      0.41       270\n",
        "\n",
        "\n",
        "Here is the confusion matrix:\n",
        "[[83  2  1  2  1  0  0]\n",
        " [14 23  1  0  0  0  0]\n",
        " [31  3 12  0  0  0  0]\n",
        " [23  0  0 11  0  0  0]\n",
        " [21  0  0  0  0  0  0]\n",
        " [19  0  0  0  0  1  0]\n",
        " [21  1  0  0  0  0  0]]\n",
        "\n",
        "The top 10 most informative features for topic code 1: \n",
        "make credit money want like people know does best yahoo\n",
        "\n",
        "The top 10 most informative features for topic code 3: \n",
        "free does web internet windows use best computer yahoo xa"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "The top 10 most informative features for topic code 2: \n",
        "new think tv did best music movie favorite song xa\n",
        "\n",
        "The top 10 most informative features for topic code 5: \n",
        "want sex know women friend boyfriend guy girl like love\n",
        "\n",
        "The top 10 most informative features for topic code 4: \n",
        "did come know education need words does college school word\n",
        "\n",
        "The top 10 most informative features for topic code 7: \n",
        "sex rid fight know cold best way way pain does best\n",
        "\n",
        "The top 10 most informative features for topic code 6: \n",
        "moon gas stars planet possible life xa world earth does\n",
        "*****\n",
        "MODEL: Linear SVC"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "The precision for this classifier is 0.61500532093\n",
        "The recall for this classifier is 0.611111111111\n",
        "The f1 for this classifier is 0.605288571293\n",
        "The accuracy for this classifier is 0.611111111111\n",
        "\n",
        "Here is the classification report:\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          1       0.56      0.63      0.59        89\n",
        "          2       0.59      0.76      0.67        38\n",
        "          3       0.68      0.61      0.64        46\n",
        "          4       0.68      0.74      0.70        34\n",
        "          5       0.58      0.33      0.42        21\n",
        "          6       0.65      0.55      0.59        20\n",
        "          7       0.64      0.41      0.50        22\n",
        "\n",
        "avg / total       0.62      0.61      0.61       270\n",
        "\n",
        "\n",
        "Here is the confusion matrix:\n",
        "[[56 12  7  6  4  2  2]\n",
        " [ 7 29  1  0  0  1  0]\n",
        " [10  3 28  2  1  2  0]\n",
        " [ 6  0  1 25  0  1  1]\n",
        " [10  2  1  0  7  0  1]\n",
        " [ 5  0  0  3  0 11  1]\n",
        " [ 6  3  3  1  0  0  9]]\n",
        "\n",
        "The top 10 most informative features for topic code 1: \n",
        "know love rich things california santa business stock job money credit\n",
        "\n",
        "The top 10 most informative features for topic code 3: \n",
        "google rss java software linux page windows internet use computer"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "The top 10 most informative features for topic code 2: \n",
        "robot film movies favorite magazine tv rock movie music song\n",
        "\n",
        "The top 10 most informative features for topic code 5: \n",
        "women date girls guy friend boyfriend girl marriage relationship love\n",
        "\n",
        "The top 10 most informative features for topic code 4: \n",
        "war book words schools university study education school word college\n",
        "\n",
        "The top 10 most informative features for topic code 7: \n",
        "dandruff rid teeth fight weight hiv aids surgery diet pain\n",
        "\n",
        "The top 10 most informative features for topic code 6: \n",
        "moon gneiss cubit syndicalist paupisi acid universe planet stars earth\n",
        "*****\n",
        "MODEL: Maximum Entropy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "The precision for this classifier is 0.660152093081\n",
        "The recall for this classifier is 0.503703703704\n",
        "The f1 for this classifier is 0.456468408562\n",
        "The accuracy for this classifier is 0.503703703704\n",
        "\n",
        "Here is the classification report:\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          1       0.40      0.82      0.54        89\n",
        "          2       0.67      0.63      0.65        38\n",
        "          3       0.84      0.35      0.49        46\n",
        "          4       0.66      0.56      0.60        34\n",
        "          5       0.67      0.10      0.17        21\n",
        "          6       1.00      0.05      0.10        20\n",
        "          7       1.00      0.05      0.09        22\n",
        "\n",
        "avg / total       0.66      0.50      0.46       270\n",
        "\n",
        "\n",
        "Here is the confusion matrix:\n",
        "[[73  7  2  6  1  0  0]\n",
        " [13 24  1  0  0  0  0]\n",
        " [25  3 16  2  0  0  0]\n",
        " [15  0  0 19  0  0  0]\n",
        " [18  1  0  0  2  0  0]\n",
        " [18  0  0  1  0  1  0]\n",
        " [19  1  0  1  0  0  1]]\n",
        "\n",
        "The top 10 most informative features for topic code 1: \n",
        "santa meaning stock business rich people buy job money credit"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "The top 10 most informative features for topic code 3: \n",
        "website page linux web software internet windows yahoo use computer\n",
        "\n",
        "The top 10 most informative features for topic code 2: \n",
        "movies magazine did xa rock tv favorite music movie song\n",
        "\n",
        "The top 10 most informative features for topic code 5: \n",
        "marriage date girls relationship friend women guy boyfriend girl love\n",
        "\n",
        "The top 10 most informative features for topic code 4: \n",
        "spanish schools language degree study words education college school word\n",
        "\n",
        "The top 10 most informative features for topic code 7: \n",
        "teeth cold cure hiv aids fight rid surgery diet pain\n",
        "\n",
        "The top 10 most informative features for topic code 6: \n",
        "life design possible world universe gas moon planet stars earth\n"
       ]
      }
     ],
     "prompt_number": 513
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#want to see if preprocessing collection helps accuracy. "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 514
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#spellchecker! tries to correct mispelled words\n",
      "#adapted from http://norvig.com/spell-correct.html\n",
      "def words(text): return re.findall('[a-z]+', text.lower()) \n",
      "\n",
      "def train(features):\n",
      "    model = collections.defaultdict(lambda: 1)\n",
      "    for f in features:\n",
      "        model[f] += 1\n",
      "    return model\n",
      "\n",
      "def get_txt_words():\n",
      "     mystring = ''\n",
      "     gstring = \" \".join(reuters.words())\n",
      "     mystring =  mystring + gstring\n",
      "     gstring = \" \".join(brown.words())\n",
      "     mystring =  mystring + gstring\n",
      "     return mystring\n",
      "\n",
      "NWORDS = train(words(get_txt_words()))\n",
      "\n",
      "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
      "\n",
      "def edits1(word):\n",
      "   splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
      "   deletes    = [a + b[1:] for a, b in splits if b]\n",
      "   transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]\n",
      "   replaces   = [a + c + b[1:] for a, b in splits for c in alphabet if b]\n",
      "   inserts    = [a + c + b     for a, b in splits for c in alphabet]\n",
      "   return set(deletes + transposes + replaces + inserts)\n",
      "\n",
      "def known_edits2(word):\n",
      "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in NWORDS)\n",
      "\n",
      "def known(words): return set(w for w in words if w in NWORDS)\n",
      "\n",
      "def correct(word):\n",
      "    candidates = known([word]) or known(edits1(word)) or known_edits2(word) or [word]\n",
      "    return max(candidates, key=NWORDS.get)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 515
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Class to pre-process the collection\n",
      "#class is full of goodies/functions for preprocessing data- remove stopwords,stemm, punk, and convert to lower, lemmatize \n",
      "class PreprocessText:\n",
      "\n",
      "    #function to remove punct. \n",
      "    def remove_punct(self, text):\n",
      "      exclude = set(string.punctuation)\n",
      "      table = string.maketrans(\"\",\"\")\n",
      "      text = text.translate(table, string.punctuation)\n",
      "      return text\n",
      "\n",
      "    #remove stopwords-> A quick way to reduce elminate words that aren't valid key words.\n",
      "    def removestopwords(self, tokens):\n",
      "      stopwords = nltk.corpus.stopwords.words('english')\n",
      "      tokens = [w for w in tokens if w.lower().strip() not in stopwords]\n",
      "      return tokens\n",
      "\n",
      "    ##lemmatize the words to reduce dimensionality. Also,option to do lemmatization based on POS. \n",
      "    #wordnet lemmatizer assumes everything is a noun unless otherwise specified, so we need to give\n",
      "    #it the wordnet pos if we don't want the default noun lookup. \n",
      "    def lemmatize(self, tokens, lemmatize_pos):\n",
      "        def get_wordnet_pos( pos_tag):\n",
      "            if pos_tag[1].startswith('J'):\n",
      "                return (pos_tag[0], wordnet.ADJ)\n",
      "            elif pos_tag[1].startswith('V'):\n",
      "                return (pos_tag[0], wordnet.VERB)\n",
      "            elif pos_tag[1].startswith('N'):\n",
      "                return (pos_tag[0], wordnet.NOUN)\n",
      "            elif pos_tag[1].startswith('R'):\n",
      "                return (pos_tag[0], wordnet.ADV)\n",
      "            else:\n",
      "                return (pos_tag[0], wordnet.NOUN)\n",
      "        lemmatizer = WordNetLemmatizer()\n",
      "        if lemmatize_pos:\n",
      "            tokens_pos = nltk.tag.pos_tag(tokens)\n",
      "            tokens_pos_wordnet = [ get_wordnet_pos(token) for token in tokens_pos]\n",
      "            tokens_lemm = [lemmatizer.lemmatize(token[0], token[1]) for token in tokens_pos_wordnet]\n",
      "        else:\n",
      "            tokens_lemm = [lemmatizer.lemmatize(token) for token in tokens] \n",
      "        return tokens_lemm\n",
      "    \n",
      "    #function that combines above functions in one routine\n",
      "    #lots of args to specify what preprocessing routine you want to use\n",
      "    def preprocess_txt(self, text, convertlower=True, nopunk=True, stopwords=True, lemmatize_doc=True, lemmatize_pos=True, stemmed=False, correct_wd = True):\n",
      "      #convert to lower\n",
      "      if convertlower:\n",
      "        text = text.lower()\n",
      "      # remove punctuation\n",
      "      if nopunk:\n",
      "        text = self.remove_punct(text)\n",
      "      #tokenize text\n",
      "      tokens = PunktWordTokenizer().tokenize(text)\n",
      "      #remove extra whitespaces\n",
      "      tokens = [token.strip() for token in tokens]\n",
      "      if correct_wd:\n",
      "          tokens = [correct(token) for token in tokens]\n",
      "      #remove stopwords\n",
      "      if stopwords:\n",
      "        tokens = self.removestopwords(tokens)\n",
      "      #lemmatize\n",
      "      if lemmatize_doc:\n",
      "        tokens = self.lemmatize(tokens,lemmatize_pos)\n",
      "      #stem\n",
      "      if stemmed:\n",
      "        porter = PorterStemmer()\n",
      "        tokens = [ porter.stem(token) for token in tokens ]\n",
      "      #combine the tokens back into a string...need to do this for the tfidf vectorizer\n",
      "      token_line = \" \".join(tokens)\n",
      "      return token_line\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 516
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##now try to pre-process the text before using the classifiers\n",
      "pt = PreprocessText()\n",
      "print yahoo_train[:1]\n",
      "#spell checker didn't work- need more training corpuss for the spell checker....\n",
      "#also spell checker slows down the pre-processing. \n",
      "yahoo_train_pr = [(pt.preprocess_txt(item[0], stemmed=False, correct_wd=False), item[1]) for item in yahoo_train ]\n",
      "print  yahoo_train_pr[:1]\n",
      "yahoo_test_pr = [(pt.preprocess_txt(item[0], stemmed=False, correct_wd = False), item[1]) for item in yahoo_test ]\n",
      "yahoo_train_dict_pr  =  OrderedDict(yahoo_train_pr)\n",
      "yahoo_test_dict_pr  =  OrderedDict(yahoo_test_pr)\n",
      "train_words_pr, train_cats_pr, test_words_pr, test_cats_pr = get_train_and_test_dicts(yahoo_train_dict_pr, yahoo_test_dict_pr)\n",
      "#create new vectorizer, this time without stop words since we removed them already\n",
      "vectorizer_pr = TfidfVectorizer(ngram_range=(1, 2), strip_accents='unicode')\n",
      "X_train_words_pr = vectorizer_pr.fit_transform(train_words_pr)\n",
      "#transformed the test data into a TF-IDF weighted word vector in the vocab space of the training data(.transform())\n",
      "X_test_words_pr = vectorizer_pr.transform(test_words_pr)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('what are your favorite colors? ', '1')]\n",
        "[('favorite color', '1')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 517
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get the bayes resuts \n",
      "bayes_classifier_pr, yahoo_bayes_predicted_pr = get_bayes(X_train_words_pr, train_cats_pr, X_test_words_pr )\n",
      "bayes_model_name_pr =  \"MODEL: Multinomial Naive Bayes\"\n",
      "evaluate_results(bayes_model_name_pr, test_cats_pr, yahoo_bayes_predicted_pr, bayes_classifier_pr, vectorizer_pr)\n",
      "#then get the SVM results:\n",
      "print \"*****\"\n",
      "svm_classifier_pr, yahoo_svm_predicted_pr = get_svm(X_train_words_pr, train_cats_pr, X_test_words_pr )\n",
      "svm_model_name_pr = \"MODEL: Linear SVC\"\n",
      "evaluate_results(svm_model_name_pr, test_cats_pr, yahoo_svm_predicted_pr, svm_classifier_pr, vectorizer_pr)\n",
      "print \"*****\"\n",
      "#then max ent results\n",
      "maxent_classifier_pr, yahoo_maxent_predicted_pr = get_maxent(X_train_words_pr, train_cats_pr, X_test_words_pr )\n",
      "maxent_model_name_pr = \"MODEL: Maximum Entropy\"\n",
      "evaluate_results(maxent_model_name_pr, test_cats_pr, yahoo_maxent_predicted_pr, maxent_classifier_pr, vectorizer_pr)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MODEL: Multinomial Naive Bayes\n",
        "The precision for this classifier is 0.582176689052\n",
        "The recall for this classifier is 0.462962962963\n",
        "The f1 for this classifier is 0.383069064165\n",
        "The accuracy for this classifier is 0.462962962963\n",
        "\n",
        "Here is the classification report:\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          1       0.38      0.96      0.54        89\n",
        "          2       0.88      0.61      0.72        38\n",
        "          3       1.00      0.20      0.33        46\n",
        "          4       0.70      0.21      0.32        34\n",
        "          5       0.00      0.00      0.00        21\n",
        "          6       1.00      0.05      0.10        20\n",
        "          7       0.00      0.00      0.00        22\n",
        "\n",
        "avg / total       0.58      0.46      0.38       270\n",
        "\n",
        "\n",
        "Here is the confusion matrix:\n",
        "[[85  2  0  2  0  0  0]\n",
        " [15 23  0  0  0  0  0]\n",
        " [35  1  9  1  0  0  0]\n",
        " [27  0  0  7  0  0  0]\n",
        " [21  0  0  0  0  0  0]\n",
        " [19  0  0  0  0  1  0]\n",
        " [22  0  0  0  0  0  0]]\n",
        "\n",
        "The top 10 most informative features for topic code 1: \n",
        "many make would yahoo people like know best find get"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "The top 10 most informative features for topic code 3: \n",
        "email website site web internet get best use yahoo computer\n",
        "\n",
        "The top 10 most informative features for topic code 2: \n",
        "think show first get like best music favorite song movie\n",
        "\n",
        "The top 10 most informative features for topic code 5: \n",
        "boyfriend relationship want get friend guy woman like girl love\n",
        "\n",
        "The top 10 most informative features for topic code 4: \n",
        "name need go come find study mean college school word\n",
        "\n",
        "The top 10 most informative features for topic code 7: \n",
        "fight surgery help best way cold know way pain best get\n",
        "\n",
        "The top 10 most informative features for topic code 6: \n",
        "possible color gas life work moon number world many earth\n",
        "*****\n",
        "MODEL: Linear SVC"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "The precision for this classifier is 0.61073593736\n",
        "The recall for this classifier is 0.611111111111\n",
        "The f1 for this classifier is 0.605048020684\n",
        "The accuracy for this classifier is 0.611111111111\n",
        "\n",
        "Here is the classification report:\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          1       0.60      0.62      0.61        89\n",
        "          2       0.60      0.71      0.65        38\n",
        "          3       0.69      0.59      0.64        46\n",
        "          4       0.64      0.82      0.72        34\n",
        "          5       0.58      0.33      0.42        21\n",
        "          6       0.53      0.45      0.49        20\n",
        "          7       0.57      0.55      0.56        22\n",
        "\n",
        "avg / total       0.61      0.61      0.61       270\n",
        "\n",
        "\n",
        "Here is the confusion matrix:\n",
        "[[55 14  6  7  2  2  3]\n",
        " [ 7 27  2  1  1  0  0]\n",
        " [ 9  3 27  3  0  4  0]\n",
        " [ 2  0  1 28  1  1  1]\n",
        " [ 9  1  0  1  7  0  3]\n",
        " [ 5  0  1  3  0  9  2]\n",
        " [ 5  0  2  1  1  1 12]]\n",
        "\n",
        "The top 10 most informative features for topic code 1: \n",
        "know love santa business favorite color tax credit become stock job money\n",
        "\n",
        "The top 10 most informative features for topic code 3: \n",
        "java linux file software window page yahoo internet use computer"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "The top 10 most informative features for topic code 2: \n",
        "show film kill favorite celebrity magazine rock music movie song\n",
        "\n",
        "The top 10 most informative features for topic code 5: \n",
        "family friend date guy boyfriend marriage woman relationship girl love\n",
        "\n",
        "The top 10 most informative features for topic code 4: \n",
        "president wood education degree handwritingwizardcom university study word college school\n",
        "\n",
        "The top 10 most informative features for topic code 7: \n",
        "scurl philtrum teeth fight hiv weight aid diet surgery pain\n",
        "\n",
        "The top 10 most informative features for topic code 6: \n",
        "paupisi gneiss syndicalist gas whats plus universe planet acid moon earth\n",
        "*****\n",
        "MODEL: Maximum Entropy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "The precision for this classifier is 0.629862108317\n",
        "The recall for this classifier is 0.533333333333\n",
        "The f1 for this classifier is 0.488203242465\n",
        "The accuracy for this classifier is 0.533333333333\n",
        "\n",
        "Here is the classification report:\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          1       0.43      0.84      0.57        89\n",
        "          2       0.74      0.66      0.69        38\n",
        "          3       0.83      0.33      0.47        46\n",
        "          4       0.66      0.68      0.67        34\n",
        "          5       0.75      0.14      0.24        21\n",
        "          6       1.00      0.10      0.18        20\n",
        "          7       0.33      0.05      0.08        22\n",
        "\n",
        "avg / total       0.63      0.53      0.49       270\n",
        "\n",
        "\n",
        "Here is the confusion matrix:\n",
        "[[75  5  0  7  1  0  1]\n",
        " [11 25  2  0  0  0  0]\n",
        " [29  1 15  1  0  0  0]\n",
        " [10  0  1 23  0  0  0]\n",
        " [16  2  0  0  3  0  0]\n",
        " [14  0  0  3  0  2  1]\n",
        " [19  1  0  1  0  0  1]]\n",
        "\n",
        "The top 10 most informative features for topic code 1: \n",
        "sell people tax business stock find become job credit money"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "The top 10 most informative features for topic code 3: \n",
        "file site website web software page internet yahoo use computer\n",
        "\n",
        "The top 10 most informative features for topic code 2: \n",
        "kill magazine celebrity tv show rock favorite music song movie\n",
        "\n",
        "The top 10 most informative features for topic code 5: \n",
        "family marriage date boyfriend relationship friend guy woman girl love\n",
        "\n",
        "The top 10 most informative features for topic code 4: \n",
        "language university wood high education degree study college school word\n",
        "\n",
        "The top 10 most informative features for topic code 7: \n",
        "cold rid treatment weight hiv fight aid diet surgery pain\n",
        "\n",
        "The top 10 most informative features for topic code 6: \n",
        "color world human universe planet gas acid number moon earth\n"
       ]
      }
     ],
     "prompt_number": 520
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get the actual test data from kaggle\n",
      "def prep_kaggle_testdata():\n",
      "    f = open('data/test.csv','rb')\n",
      "    test_kaggle_raw = f.read()\n",
      "    f.close() \n",
      "    test_kaggle_split_lines = test_kaggle_raw.split('\\n')\n",
      "    test_kaggle_split =  [line.split(\",\") for line in  test_kaggle_split_lines]\n",
      "    test_kaggle_tuples  = [ ( line[0], \" \".join(line[1:]) )  for line in test_kaggle_split]\n",
      "    return test_kaggle_tuples[1: ]   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 521
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kaggle_test = prep_kaggle_testdata()\n",
      "kaggle_test_pr = [(pt.preprocess_txt(item[0], stemmed=False, correct_wd = False), item[1]) for item in kaggle_test ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 522
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_kaggle_test(kaggle_test):\n",
      "    kaggle_test_keys =  np.array(kaggle_test.keys())\n",
      "    kaggle_test_vals =  np.array(kaggle_test.values())\n",
      "    return kaggle_test_keys, kaggle_test_vals"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 523
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##prepare the kaggle test words- turn into a ordered dict \n",
      "kaggle_test_dict_pr =  OrderedDict(kaggle_test_pr)\n",
      "kaggle_test_number_pr, kaggle_test_words_pr =  get_kaggle_test(kaggle_test_dict_pr)\n",
      "#transformed the test data into a TF-IDF weighted word vector in the vocab space of the training data(.transform())\n",
      "X_kaggle_test_words_pr = vectorizer_pr.transform(kaggle_test_words_pr)\n",
      "kaggle_svm_predicted = svm_classifier_pr.predict(X_kaggle_test_words_pr)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 524
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print kaggle_test_pr[726]\n",
      "print kaggle_test_words_pr[726]\n",
      "print kaggle_test_number_pr[726]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('727', 'what is your favorite color? ')\n",
        "what is your favorite color? \n",
        "727\n"
       ]
      }
     ],
     "prompt_number": 525
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#transformed the test data into a TF-IDF weighted word vector in the vocab space of the training data(.transform())\n",
      "X_kaggle_test_words_pr = vectorizer_pr.transform(kaggle_test_words_pr)\n",
      "kaggle_svm_predicted_pr = svm_classifier_pr.predict(X_kaggle_test_words_pr)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 526
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#now combined the values back together again....\n",
      "kaggle_finished_dict =  zip(kaggle_test_number_pr, kaggle_svm_predicted_pr )\n",
      "#print the results into a file: \n",
      "\n",
      "#delete the keys with no items\n",
      "f = open('results_2.csv', 'wb')\n",
      "f.write (\"Id,Category\\n\")\n",
      "for k,v in kaggle_finished_dict:\n",
      "    f.write(k +\",\" + v + \"\\n\")\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 527
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 527
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}