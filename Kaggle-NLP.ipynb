{
 "metadata": {
  "name": "",
  "signature": "sha256:d67cffb8104a3d5e28515468833a8354e980d0f1ad15fd5f71dba776bac34860"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk \n",
      "import random\n",
      "from random import shuffle \n",
      "from collections import Counter\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "from nltk.tokenize.punkt import PunktWordTokenizer\n",
      "import nltk.tag, nltk.data\n",
      "from nltk.corpus import wordnet\n",
      "from nltk.stem.porter import PorterStemmer\n",
      "import numpy as np\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn import metrics\n",
      "from operator import itemgetter\n",
      "from sklearn.metrics import classification_report\n",
      "import csv\n",
      "import os\n",
      "import collections\n",
      "import operator\n",
      "from collections import OrderedDict\n",
      "import string"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 323
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#function to break up the data into training \n",
      "def prep_data():\n",
      "    f = open('data/train.txt','rb')\n",
      "    train_raw = f.read()\n",
      "    f.close() \n",
      "    train_split = train_raw.split('\\n')\n",
      "    train_tuples = [ (line[2:], line[0]) for line in train_split if line != '']\n",
      "    shuffle(train_tuples)\n",
      "    total_size = len(train_tuples)\n",
      "    train_size = int(total_size * 0.9) \n",
      "    return train_tuples[:train_size], train_tuples[train_size:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 325
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get the test and training sets, print the size\n",
      "yahoo_train, yahoo_test = prep_data()\n",
      "print 'The training set size is ' + str(len(yahoo_train))\n",
      "print 'The test set size is ' + str(len(yahoo_test))  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The training set size is 2428\n",
        "The test set size is 270\n"
       ]
      }
     ],
     "prompt_number": 326
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#transform the training set into a dict, get the keys and vals for the tet and train\n",
      "def get_train_and_test_dicts(yahoo_train_dict, yahoo_test_dict):\n",
      "    yahoo_train_keys = yahoo_train_dict.keys()\n",
      "    #yahoo_train_keys = [unicode(word) for word in yahoo_train_keys]\n",
      "    yahoo_train_vals = yahoo_train_dict.values()\n",
      "    yahoo_test_keys = yahoo_test_dict.keys()\n",
      "    #yahoo_test_keys = [unicode(word) for word in yahoo_test_keys ]\n",
      "    yahoo_test_vals = yahoo_test_dict.values()\n",
      "    return np.array(yahoo_train_keys), np.array(yahoo_train_vals), np.array(yahoo_test_keys), np.array(yahoo_test_vals)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 327
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Created a vocabulary from the training data (the fit in .fit_transform())\n",
      "#then turn the words into a TF-IDF weighted word vector(the transform in .fit_transform())\n",
      "#Convert a collection of raw documents to a matrix of TF-IDF features\n",
      "\n",
      "#lol. Don't necessarily need to do preprocessing- Scipylearn can do all the preprocessing for you\n",
      "#want to use tfidf vectors to just grab the words that are most relevant\n",
      "#Convert a collection of raw documents to a matrix of TF-IDF features.\n",
      "#http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
      "\n",
      "vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english',  strip_accents='unicode')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 341
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#split the test and train into numpy arrays so they can be vectorized and trained with\n",
      "yahoo_train_dict  =  OrderedDict(yahoo_train)\n",
      "yahoo_test_dict  =  OrderedDict(yahoo_test)\n",
      "train_words, train_cats, test_words, test_cats = get_train_and_test_dicts(yahoo_train_dict, yahoo_test_dict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 342
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#example of how the vectorizer works\n",
      "tes_string = train_words[0]\n",
      "print \"Example string: \" + test_string\n",
      "print \"Preprocessed string: \" + vectorizer.build_preprocessor()(test_string)\n",
      "print \"Tokenized string:\" + str(vectorizer.build_tokenizer()(test_string))\n",
      "print \"N-gram data string:\" + str(vectorizer.build_analyzer()(test_string))\n",
      "print \"\\n\"\n",
      "\n",
      "#Created a vocabulary from the training data (the fit in .fit_transform())\",\n",
      "#then turn the words into a TF-IDF weighted word vector(the transform in .fit_transform())\"\n",
      "X_train_words = vectorizer.fit_transform(train_words)\n",
      "\n",
      "#transformed the test data into a TF-IDF weighted word vector in the vocab space of the training data(.transform())\n",
      "X_test_words = vectorizer.transform(test_words)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Example string: capital of finland? \n",
        "Preprocessed string: capital of finland? \n",
        "Tokenized string:[u'capital', u'of', u'finland']\n",
        "N-gram data string:[u'capital', u'finland', u'capital finland']\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 343
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#function to evaluate results of each model\n",
      "def evaluate_results(model_name, test_cats, predicted, myclassifier, myvectorizer, prwords = False,):\n",
      "        print model_name\n",
      "        print 'The precision for this classifier is ' + str(metrics.precision_score(test_cats, predicted))\n",
      "        print 'The recall for this classifier is ' + str(metrics.recall_score(test_cats, predicted))\n",
      "        print 'The f1 for this classifier is ' + str(metrics.f1_score(test_cats, predicted))\n",
      "        print 'The accuracy for this classifier is ' + str(metrics.accuracy_score(test_cats, predicted))\n",
      "        print '\\nHere is the classification report:'\n",
      "        print classification_report(test_cats, predicted)\n",
      "        print '\\nHere is the confusion matrix:'\n",
      "        print metrics.confusion_matrix(test_cats, predicted)\n",
      "        ##print the top 10 words for each category\n",
      "        if prwords == True:\n",
      "            N = 10\n",
      "            vocabulary = np.array([t for t, i in sorted(myvectorizer.vocabulary_.iteritems(), key=itemgetter(1))])\n",
      "            for i, label in enumerate(set(test_cats)):\n",
      "                topN = np.argsort(myclassifier.coef_[i])[-N:]\n",
      "                print \"\\nThe top %d most informative features for topic code %s: \\n%s\" % (N, label, \" \".join(vocabulary[topN]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 389
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##bayes classifier\n",
      "def get_bayes(X_train_words, train_cats, X_test_words):\n",
      "    #build the classifier\n",
      "    bayes_classifier = MultinomialNB().fit(X_train_words,train_cats )\n",
      "    yahoo_bayes_predicted = bayes_classifier.predict(X_test_words)\n",
      "    return bayes_classifier, yahoo_bayes_predicted"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 377
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#SVM classifier\n",
      "def get_svm(X_train_words, train_cats, X_test_words):\n",
      "    svm_classifier = LinearSVC().fit(X_train_words, train_cats)\n",
      "    yahoo_svm_predicted = svm_classifier.predict(X_test_words)\n",
      "    return svm_classifier, yahoo_svm_predicted"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 378
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "###now try max ent as a classifier\n",
      "def get_maxent(X_train_words, train_cats, X_test_words):\n",
      "    maxent_classifier = LogisticRegression().fit(X_train_words, train_cats)\n",
      "    yahoo_maxent_predicted = maxent_classifier.predict(X_test_words)\n",
      "    return maxent_classifier, yahoo_maxent_predicted"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 379
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get the bayes resuts \n",
      "bayes_classifier, yahoo_bayes_predicted = get_bayes(X_train_words, train_cats, X_test_words )\n",
      "bayes_model_name =  \"MODEL: Multinomial Naive Bayes\"\n",
      "evaluate_results(bayes_model_name, test_cats, yahoo_bayes_predicted, bayes_classifier, vectorizer)\n",
      "#then get the SVM results:\n",
      "print \"*****\"\n",
      "svm_classifier, yahoo_svm_predicted = get_svm(X_train_words, train_cats, X_test_words )\n",
      "svm_model_name = \"MODEL: Linear SVC\"\n",
      "evaluate_results(svm_model_name, test_cats, yahoo_svm_predicted, svm_classifier, vectorizer)\n",
      "print \"*****\"\n",
      "#then max ent results\n",
      "maxent_classifier, yahoo_maxent_predicted = get_maxent(X_train_words, train_cats, X_test_words )\n",
      "maxent_model_name = \"MODEL: Maximum Entropy\"\n",
      "evaluate_results(maxent_model_name, test_cats, yahoo_maxent_predicted, maxent_classifier, vectorizer)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MODEL: Multinomial Naive Bayes\n",
        "The precision for this classifier is 0.554263523801\n",
        "The recall for this classifier is 0.437037037037\n",
        "The f1 for this classifier is 0.367935915336\n",
        "The accuracy for this classifier is 0.437037037037\n",
        "\n",
        "Here is the classification report:\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          1       0.33      0.95      0.49        75\n",
        "          2       0.79      0.57      0.67        47\n",
        "          3       1.00      0.27      0.42        45\n",
        "          4       0.78      0.28      0.41        25\n",
        "          5       0.00      0.00      0.00        30\n",
        "          6       1.00      0.04      0.08        23\n",
        "          7       0.00      0.00      0.00        25\n",
        "\n",
        "avg / total       0.55      0.44      0.37       270\n",
        "\n",
        "\n",
        "Here is the confusion matrix:\n",
        "[[71  3  0  1  0  0  0]\n",
        " [20 27  0  0  0  0  0]\n",
        " [32  1 12  0  0  0  0]\n",
        " [18  0  0  7  0  0  0]\n",
        " [29  1  0  0  0  0  0]\n",
        " [21  0  0  1  0  1  0]\n",
        " [23  2  0  0  0  0  0]]\n",
        "*****\n",
        "MODEL: Linear SVC"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "The precision for this classifier is 0.592136144557\n",
        "The recall for this classifier is 0.581481481481\n",
        "The f1 for this classifier is 0.57739007712\n",
        "The accuracy for this classifier is 0.581481481481\n",
        "\n",
        "Here is the classification report:\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          1       0.44      0.57      0.50        75\n",
        "          2       0.71      0.77      0.73        47\n",
        "          3       0.69      0.60      0.64        45\n",
        "          4       0.67      0.80      0.73        25\n",
        "          5       0.53      0.30      0.38        30\n",
        "          6       0.75      0.52      0.62        23\n",
        "          7       0.50      0.40      0.44        25\n",
        "\n",
        "avg / total       0.59      0.58      0.58       270\n",
        "\n",
        "\n",
        "Here is the confusion matrix:\n",
        "[[43 10  7  4  4  2  5]\n",
        " [ 9 36  0  1  0  0  1]\n",
        " [11  0 27  2  4  0  1]\n",
        " [ 4  0  1 20  0  0  0]\n",
        " [13  1  3  0  9  2  2]\n",
        " [ 6  1  1  2  0 12  1]\n",
        " [11  3  0  1  0  0 10]]\n",
        "*****\n",
        "MODEL: Maximum Entropy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "The precision for this classifier is 0.64238458601\n",
        "The recall for this classifier is 0.518518518519\n",
        "The f1 for this classifier is 0.469046976758\n",
        "The accuracy for this classifier is 0.518518518519\n",
        "\n",
        "Here is the classification report:\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          1       0.38      0.89      0.53        75\n",
        "          2       0.76      0.74      0.75        47\n",
        "          3       0.95      0.40      0.56        45\n",
        "          4       0.74      0.56      0.64        25\n",
        "          5       1.00      0.07      0.12        30\n",
        "          6       0.80      0.17      0.29        23\n",
        "          7       0.00      0.00      0.00        25\n",
        "\n",
        "avg / total       0.64      0.52      0.47       270\n",
        "\n",
        "\n",
        "Here is the confusion matrix:\n",
        "[[67  5  0  2  0  0  1]\n",
        " [12 35  0  0  0  0  0]\n",
        " [25  0 18  2  0  0  0]\n",
        " [10  1  0 14  0  0  0]\n",
        " [25  1  1  0  2  1  0]\n",
        " [17  1  0  1  0  4  0]\n",
        " [22  3  0  0  0  0  0]]\n"
       ]
      }
     ],
     "prompt_number": 390
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#want to see if preprocessing collection helps accuracy. "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 359
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Class to pre-process the collection\n",
      "#class is full of goodies/functions for preprocessing data- remove stopwords,stemm, punk, and convert to lower, lemmatize \n",
      "class PreprocessText:\n",
      "\n",
      "    #function to remove punct. \n",
      "    def remove_punct(self, text):\n",
      "      exclude = set(string.punctuation)\n",
      "      table = string.maketrans(\"\",\"\")\n",
      "      text = text.translate(table, string.punctuation)\n",
      "      return text\n",
      "\n",
      "    #remove stopwords-> A quick way to reduce elminate words that aren't valid key words.\n",
      "    def removestopwords(self, tokens):\n",
      "      stopwords = nltk.corpus.stopwords.words('english')\n",
      "      tokens = [w for w in tokens if w.lower().strip() not in stopwords]\n",
      "      return tokens\n",
      "\n",
      "    ##lemmatize the words to reduce dimensionality. Also,option to do lemmatization based on POS. \n",
      "    #wordnet lemmatizer assumes everything is a noun unless otherwise specified, so we need to give\n",
      "    #it the wordnet pos if we don't want the default noun lookup. \n",
      "    def lemmatize(self, tokens, lemmatize_pos):\n",
      "        def get_wordnet_pos( pos_tag):\n",
      "            if pos_tag[1].startswith('J'):\n",
      "                return (pos_tag[0], wordnet.ADJ)\n",
      "            elif pos_tag[1].startswith('V'):\n",
      "                return (pos_tag[0], wordnet.VERB)\n",
      "            elif pos_tag[1].startswith('N'):\n",
      "                return (pos_tag[0], wordnet.NOUN)\n",
      "            elif pos_tag[1].startswith('R'):\n",
      "                return (pos_tag[0], wordnet.ADV)\n",
      "            else:\n",
      "                return (pos_tag[0], wordnet.NOUN)\n",
      "        lemmatizer = WordNetLemmatizer()\n",
      "        if lemmatize_pos:\n",
      "            tokens_pos = nltk.tag.pos_tag(tokens)\n",
      "            tokens_pos_wordnet = [ get_wordnet_pos(token) for token in tokens_pos]\n",
      "            tokens_lemm = [lemmatizer.lemmatize(token[0], token[1]) for token in tokens_pos_wordnet]\n",
      "        else:\n",
      "            tokens_lemm = [lemmatizer.lemmatize(token) for token in tokens] \n",
      "        return tokens_lemm\n",
      "    \n",
      "    #function that combines above functions in one routine\n",
      "    #lots of args to specify what preprocessing routine you want to use\n",
      "    def preprocess_txt(self, text, convertlower=True, nopunk=True, stopwords=True, lemmatize_doc=True, lemmatize_pos=True, stemmed=False):\n",
      "      #convert to lower\n",
      "      if convertlower:\n",
      "        text = text.lower()\n",
      "      # remove punctuation\n",
      "      if nopunk:\n",
      "        text = self.remove_punct(text)\n",
      "      #tokenize text\n",
      "      tokens = PunktWordTokenizer().tokenize(text)\n",
      "      #remove stopwords\n",
      "      if stopwords:\n",
      "        tokens = self.removestopwords(tokens)\n",
      "      #lemmatize\n",
      "      if lemmatize_doc:\n",
      "        tokens = self.lemmatize(tokens,lemmatize_pos)\n",
      "      #stem\n",
      "      if stemmed:\n",
      "        porter = PorterStemmer()\n",
      "        tokens = [ porter.stem(token) for token in tokens ]\n",
      "      #combine the tokens back into a string...need to do this for the tfidf vectorizer\n",
      "      token_line = \" \".join(tokens)\n",
      "      return token_line\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 360
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##now try the \n",
      "pt = PreprocessText()\n",
      "print yahoo_train[:1]\n",
      "yahoo_train_pr = [(pt.preprocess_txt(item[0], stemmed=False), item[1]) for item in yahoo_train ]\n",
      "print  yahoo_train_pr[:1]\n",
      "yahoo_test_pr = [(pt.preprocess_txt(item[0], stemmed=False), item[1]) for item in yahoo_test ]\n",
      "yahoo_train_dict_pr  =  OrderedDict(yahoo_train_pr)\n",
      "yahoo_test_dict_pr  =  OrderedDict(yahoo_test_pr)\n",
      "train_words_pr, train_cats_pr, test_words_pr, test_cats_pr = get_train_and_test_dicts(yahoo_train_dict_pr, yahoo_test_dict_pr)\n",
      "#create new vectorizer, this time without stop words since we removed them already\n",
      "vectorizer_pr = TfidfVectorizer(ngram_range=(1, 2), strip_accents='unicode')\n",
      "X_train_words_pr = vectorizer_pr.fit_transform(train_words_pr)\n",
      "#transformed the test data into a TF-IDF weighted word vector in the vocab space of the training data(.transform())\n",
      "X_test_words_pr = vectorizer_pr.transform(test_words_pr)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('capital of finland? ', '1')]\n",
        "[('capital finland', '1')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 405
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get the bayes resuts \n",
      "bayes_classifier_pr, yahoo_bayes_predicted_pr = get_bayes(X_train_words_pr, train_cats_pr, X_test_words_pr )\n",
      "bayes_model_name =  \"MODEL: Multinomial Naive Bayes\"\n",
      "evaluate_results(bayes_model_name, test_cats_pr, yahoo_bayes_predicted_pr, bayes_classifier_pr, X_test_words_pr, vectorizer_pr)\n",
      "#then get the SVM results:\n",
      "print \"*****\"\n",
      "svm_classifier_pr, yahoo_svm_predicted_pr = get_svm(X_train_words_pr, train_cats_pr, X_test_words_pr )\n",
      "svm_model_name = \"MODEL: Linear SVC\"\n",
      "evaluate_results(svm_model_name, test_cats_pr, yahoo_svm_predicted_pr, svm_classifier_pr, vectorizer_pr)\n",
      "print \"*****\"\n",
      "#then max ent results\n",
      "maxent_classifier_pr, yahoo_maxent_predicted_pr = get_maxent(X_train_words_pr, train_cats_pr, X_test_words_pr )\n",
      "maxent_model_name = \"MODEL: Maximum Entropy\"\n",
      "evaluate_results(maxent_model_name, test_cats_pr, yahoo_maxent_predicted_pr, maxent_classifier_pr, vectorizer_pr)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MODEL: Multinomial Naive Bayes\n",
        "The precision for this classifier is 0.446794890128\n",
        "The recall for this classifier is 0.396296296296\n",
        "The f1 for this classifier is 0.322950737534\n",
        "The accuracy for this classifier is 0.396296296296\n",
        "\n",
        "Here is the classification report:\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          1       0.31      0.92      0.46        75\n",
        "          2       0.73      0.47      0.57        47\n",
        "          3       1.00      0.24      0.39        45\n",
        "          4       0.71      0.20      0.31        25\n",
        "          5       0.00      0.00      0.00        30\n",
        "          6       0.00      0.00      0.00        23\n",
        "          7       0.00      0.00      0.00        25\n",
        "\n",
        "avg / total       0.45      0.40      0.32       270\n",
        "\n",
        "\n",
        "Here is the confusion matrix:\n",
        "[[69  5  0  1  0  0  0]\n",
        " [25 22  0  0  0  0  0]\n",
        " [31  2 11  1  0  0  0]\n",
        " [20  0  0  5  0  0  0]\n",
        " [29  1  0  0  0  0  0]\n",
        " [23  0  0  0  0  0  0]\n",
        " [25  0  0  0  0  0  0]]\n",
        "*****\n",
        "MODEL: Linear SVC"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "The precision for this classifier is 0.629066747747\n",
        "The recall for this classifier is 0.622222222222\n",
        "The f1 for this classifier is 0.612946188649\n",
        "The accuracy for this classifier is 0.622222222222\n",
        "\n",
        "Here is the classification report:\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          1       0.51      0.64      0.57        75\n",
        "          2       0.66      0.83      0.74        47\n",
        "          3       0.80      0.62      0.70        45\n",
        "          4       0.70      0.84      0.76        25\n",
        "          5       0.53      0.30      0.38        30\n",
        "          6       0.70      0.61      0.65        23\n",
        "          7       0.60      0.36      0.45        25\n",
        "\n",
        "avg / total       0.63      0.62      0.61       270\n",
        "\n",
        "\n",
        "Here is the confusion matrix:\n",
        "[[48 12  3  3  5  2  2]\n",
        " [ 7 39  0  0  1  0  0]\n",
        " [11  0 28  3  1  1  1]\n",
        " [ 4  0  0 21  0  0  0]\n",
        " [11  5  1  0  9  2  2]\n",
        " [ 5  0  0  3  0 14  1]\n",
        " [ 8  3  3  0  1  1  9]]\n",
        "*****\n",
        "MODEL: Maximum Entropy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "The precision for this classifier is 0.648188090051\n",
        "The recall for this classifier is 0.522222222222\n",
        "The f1 for this classifier is 0.484702301582\n",
        "The accuracy for this classifier is 0.522222222222\n",
        "\n",
        "Here is the classification report:\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          1       0.37      0.84      0.51        75\n",
        "          2       0.73      0.70      0.72        47\n",
        "          3       0.95      0.42      0.58        45\n",
        "          4       0.68      0.68      0.68        25\n",
        "          5       1.00      0.13      0.24        30\n",
        "          6       1.00      0.22      0.36        23\n",
        "          7       0.00      0.00      0.00        25\n",
        "\n",
        "avg / total       0.65      0.52      0.48       270\n",
        "\n",
        "\n",
        "Here is the confusion matrix:\n",
        "[[63  7  1  3  0  0  1]\n",
        " [14 33  0  0  0  0  0]\n",
        " [23  0 19  3  0  0  0]\n",
        " [ 8  0  0 17  0  0  0]\n",
        " [24  2  0  0  4  0  0]\n",
        " [15  1  0  2  0  5  0]\n",
        " [23  2  0  0  0  0  0]]\n"
       ]
      }
     ],
     "prompt_number": 406
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get the actual test data from kaggle\n",
      "def prep_kaggle_testdata():\n",
      "    f = open('data/test.csv','rb')\n",
      "    test_kaggle_raw = f.read()\n",
      "    f.close() \n",
      "    test_kaggle_split_lines = test_kaggle_raw.split('\\n')\n",
      "    test_kaggle_split =  [line.split(\",\") for line in  test_kaggle_split_lines]\n",
      "    test_kaggle_tuples  = [ (\" \".join(line[1:]), line[0]) for line in test_kaggle_split]\n",
      "    return test_kaggle_tuples[1: ]   \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 304
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kaggle_test = prep_kaggle_testdata()\n",
      "print kaggle_test[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(\"why doesn't an optical mouse work on a glass table? or even on some surfaces?\", '1'), ('cascading style sheets what are the pros and cons for web developers when using css', '2'), ('in the san francisco bay area  does it make sense to rent or buy ? the prices of rent and the price of buying does not make sense to me  mostly the rent will not cover the mortgage . is it better to rent a house or to buy?', '3'), ('why is it called heavy metal music? why is it called heavy metal music?', '4'), (\"what's the best way to clean a keyboard? i have very small stuff stuck under my keyboard and it prevents it to from working. what will be the be&#xa;st way to clean it?\", '5'), ('why do people blush when they are embarrassed? why do people blush when they are embarrassed?', '6'), ('what is happiness? what is happiness?', '7'), ('can boys and girls be just friends? can boys and girls be just friends and never lovers?&#xd;&lt;br&gt;&lt;br&gt;please no answer from - when harry met sally.&#xd;&lt;br&gt;&lt;br&gt;any studies by kinsey?  i think he failed this test by the way.....', '8'), ('what is the origin of foobar? i want to know the meaning of the word and how to explain to my friends.', '9'), ('how the human species evolved? how the human species evolved?', '10')]\n"
       ]
      }
     ],
     "prompt_number": 220
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_kaggle_test(kaggle_test):\n",
      "    kaggle_test_keys =  np.array(kaggle_test_dict.keys())\n",
      "    kaggle_test_vals =  np.array(kaggle_test_dict.values())\n",
      "    return kaggle_test_keys, kaggle_test_vals"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 238
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##prepare the kaggle test words- turn into a ordered dict \n",
      "kaggle_test_dict =  OrderedDict(kaggle_test)\n",
      "kaggle_test_words, kaggle_test_number =  get_kaggle_test(kaggle_test_dict)\n",
      "#transformed the test data into a TF-IDF weighted word vector in the vocab space of the training data(.transform())\n",
      "X_kaggle_test_words = vectorizer.transform(kaggle_test_words)\n",
      "kaggle_svm_predicted = svm_classifier.predict(X_kaggle_test_words)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 244
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#now combined the values back together again....\n",
      "kaggle_finished_dict =  zip(kaggle_test_number, kaggle_svm_predicted )\n",
      "#print the results into a file: \n",
      "\n",
      "#delete the keys with no items\n",
      "for k,v in kaggle_finished_dict[:1874]:\n",
      "    print k, v\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 1\n",
        "2 2\n",
        "3 1\n",
        "4 7\n",
        "5 1\n",
        "6 1\n",
        "7 1\n",
        "8 4\n",
        "9 1\n",
        "10 7\n",
        "11 1\n",
        "12 2\n",
        "13 3\n",
        "14 1\n",
        "15 1\n",
        "16 7\n",
        "17 5\n",
        "18 7\n",
        "19 7\n",
        "20 3\n",
        "21 1\n",
        "22 1\n",
        "23 5\n",
        "24 3\n",
        "25 1\n",
        "26 1\n",
        "27 2\n",
        "28 5\n",
        "29 2\n",
        "30 2\n",
        "31 1\n",
        "32 5\n",
        "33 1\n",
        "34 2\n",
        "35 5\n",
        "36 7\n",
        "37 5\n",
        "38 2\n",
        "39 1\n",
        "40 6\n",
        "41 1\n",
        "42 3\n",
        "43 7\n",
        "44 7\n",
        "45 5\n",
        "46 4\n",
        "47 1\n",
        "48 3\n",
        "49 6\n",
        "50 1\n",
        "51 3\n",
        "52 3\n",
        "53 7\n",
        "54 1\n",
        "55 1\n",
        "56 2\n",
        "57 3\n",
        "58 1\n",
        "59 7\n",
        "60 6\n",
        "61 3\n",
        "62 1\n",
        "63 1\n",
        "64 2\n",
        "65 1\n",
        "66 3\n",
        "67 2\n",
        "68 5\n",
        "69 3\n",
        "70 5\n",
        "71 3\n",
        "72 1\n",
        "73 5\n",
        "74 1\n",
        "75 4\n",
        "76 1\n",
        "77 1\n",
        "78 3\n",
        "79 5\n",
        "80 3\n",
        "81 1\n",
        "82 1\n",
        "83 4\n",
        "84 1\n",
        "85 1\n",
        "86 1\n",
        "87 4\n",
        "88 2\n",
        "89 1\n",
        "90 6\n",
        "91 3\n",
        "92 2\n",
        "93 1\n",
        "94 5\n",
        "95 5\n",
        "96 1\n",
        "97 3\n",
        "98 5\n",
        "99 3\n",
        "100 1\n",
        "101 2\n",
        "102 1\n",
        "103 4\n",
        "104 2\n",
        "105 5\n",
        "106 4\n",
        "107 1\n",
        "108 1\n",
        "109 1\n",
        "110 2\n",
        "111 1\n",
        "112 1\n",
        "113 3\n",
        "114 3\n",
        "115 1\n",
        "116 5\n",
        "117 5\n",
        "118 4\n",
        "119 1\n",
        "120 1\n",
        "121 1\n",
        "122 1\n",
        "123 1\n",
        "124 3\n",
        "125 5\n",
        "126 1\n",
        "127 1\n",
        "128 3\n",
        "129 3\n",
        "130 6\n",
        "131 4\n",
        "132 3\n",
        "133 1\n",
        "134 7\n",
        "135 5\n",
        "136 2\n",
        "137 2\n",
        "138 2\n",
        "139 5\n",
        "140 4\n",
        "141 1\n",
        "142 1\n",
        "143 5\n",
        "144 3\n",
        "145 6\n",
        "146 1\n",
        "147 1\n",
        "148 4\n",
        "149 2\n",
        "150 2\n",
        "151 5\n",
        "152 2\n",
        "153 2\n",
        "154 5\n",
        "155 2\n",
        "156 1\n",
        "157 4\n",
        "158 5\n",
        "159 3\n",
        "160 1\n",
        "161 5\n",
        "162 1\n",
        "163 1\n",
        "164 1\n",
        "165 1\n",
        "166 4\n",
        "167 1\n",
        "168 2\n",
        "169 4\n",
        "170 3\n",
        "171 5\n",
        "172 4\n",
        "173 7\n",
        "174 1\n",
        "175 2\n",
        "176 1\n",
        "177 7\n",
        "178 1\n",
        "179 2\n",
        "180 3\n",
        "181 4\n",
        "182 1\n",
        "183 4\n",
        "184 3\n",
        "185 1\n",
        "186 1\n",
        "187 7\n",
        "188 2\n",
        "189 6\n",
        "190 2\n",
        "191 1\n",
        "192 3\n",
        "193 1\n",
        "194 5\n",
        "195 5\n",
        "196 1\n",
        "197 4\n",
        "198 2\n",
        "199 3\n",
        "200 1\n",
        "201 1\n",
        "202 1\n",
        "203 4\n",
        "204 1\n",
        "205 2\n",
        "206 4\n",
        "207 3\n",
        "208 3\n",
        "209 2\n",
        "210 7\n",
        "211 6\n",
        "212 2\n",
        "213 3\n",
        "214 2\n",
        "215 1\n",
        "216 1\n",
        "217 5\n",
        "218 1\n",
        "219 1\n",
        "220 1\n",
        "221 1\n",
        "222 3\n",
        "223 7\n",
        "224 4\n",
        "225 2\n",
        "226 1\n",
        "227 5\n",
        "228 2\n",
        "229 1\n",
        "230 1\n",
        "231 5\n",
        "232 1\n",
        "233 3\n",
        "234 1\n",
        "235 5\n",
        "236 3\n",
        "237 1\n",
        "238 3\n",
        "239 2\n",
        "240 2\n",
        "241 5\n",
        "242 5\n",
        "243 3\n",
        "244 4\n",
        "245 5\n",
        "246 6\n",
        "247 2\n",
        "248 1\n",
        "249 3\n",
        "250 2\n",
        "251 1\n",
        "252 1\n",
        "253 3\n",
        "254 1\n",
        "255 1\n",
        "256 2\n",
        "257 3\n",
        "258 2\n",
        "259 2\n",
        "260 1\n",
        "261 1\n",
        "262 5\n",
        "263 1\n",
        "264 2\n",
        "265 1\n",
        "266 5\n",
        "267 1\n",
        "268 3\n",
        "269 1\n",
        "270 1\n",
        "271 5\n",
        "272 1\n",
        "273 2\n",
        "274 4\n",
        "275 5\n",
        "276 4\n",
        "277 1\n",
        "278 1\n",
        "279 1\n",
        "280 2\n",
        "281 3\n",
        "282 7\n",
        "283 2\n",
        "284 7\n",
        "285 6\n",
        "286 4\n",
        "287 4\n",
        "288 4\n",
        "289 1\n",
        "290 1\n",
        "291 1\n",
        "292 1\n",
        "293 1\n",
        "294 4\n",
        "295 2\n",
        "296 4\n",
        "297 3\n",
        "298 1\n",
        "299 2\n",
        "300 1\n",
        "301 2\n",
        "302 6\n",
        "303 6\n",
        "304 1\n",
        "305 3\n",
        "306 3\n",
        "307 1\n",
        "308 4\n",
        "309 1\n",
        "310 1\n",
        "311 1\n",
        "312 2\n",
        "313 1\n",
        "314 4\n",
        "315 1\n",
        "316 2\n",
        "317 2\n",
        "318 1\n",
        "319 7\n",
        "320 1\n",
        "321 1\n",
        "322 4\n",
        "323 6\n",
        "324 4\n",
        "325 7\n",
        "326 1\n",
        "327 2\n",
        "328 1\n",
        "329 3\n",
        "330 2\n",
        "331 4\n",
        "332 2\n",
        "333 1\n",
        "334 1\n",
        "335 2\n",
        "336 3\n",
        "337 3\n",
        "338 1\n",
        "339 4\n",
        "340 5\n",
        "341 1\n",
        "342 4\n",
        "343 1\n",
        "344 4\n",
        "345 5\n",
        "346 4\n",
        "347 1\n",
        "348 5\n",
        "349 4\n",
        "350 1\n",
        "351 4\n",
        "352 1\n",
        "353 5\n",
        "354 2\n",
        "355 2\n",
        "356 3\n",
        "357 3\n",
        "358 1\n",
        "359 5\n",
        "360 1\n",
        "361 7\n",
        "362 2\n",
        "363 4\n",
        "364 1\n",
        "365 1\n",
        "366 4\n",
        "367 3\n",
        "368 2\n",
        "369 1\n",
        "370 2\n",
        "371 5\n",
        "372 7\n",
        "373 5\n",
        "374 3\n",
        "375 2\n",
        "376 1\n",
        "377 2\n",
        "378 6\n",
        "379 3\n",
        "380 4\n",
        "381 6\n",
        "382 3\n",
        "383 1\n",
        "384 5\n",
        "385 1\n",
        "386 3\n",
        "387 6\n",
        "388 2\n",
        "389 2\n",
        "390 3\n",
        "391 3\n",
        "392 1\n",
        "393 1\n",
        "394 6\n",
        "395 2\n",
        "396 3\n",
        "397 3\n",
        "398 3\n",
        "399 3\n",
        "400 2\n",
        "401 6\n",
        "402 3\n",
        "403 4\n",
        "404 2\n",
        "405 2\n",
        "406 4\n",
        "407 4\n",
        "408 1\n",
        "409 2\n",
        "410 2\n",
        "411 1\n",
        "412 4\n",
        "413 1\n",
        "414 2\n",
        "415 4\n",
        "416 6\n",
        "417 2\n",
        "418 1\n",
        "419 2\n",
        "420 4\n",
        "421 1\n",
        "422 1\n",
        "423 1\n",
        "424 1\n",
        "425 1\n",
        "426 4\n",
        "427 4\n",
        "428 7\n",
        "429 1\n",
        "430 1\n",
        "431 4\n",
        "432 2\n",
        "433 2\n",
        "434 3\n",
        "435 2\n",
        "436 1\n",
        "437 1\n",
        "438 1\n",
        "439 7\n",
        "440 2\n",
        "441 1\n",
        "442 2\n",
        "443 4\n",
        "444 6\n",
        "445 1\n",
        "446 2\n",
        "447 7\n",
        "448 5\n",
        "449 6\n",
        "450 1\n",
        "451 3\n",
        "452 2\n",
        "453 4\n",
        "454 4\n",
        "455 2\n",
        "456 1\n",
        "457 5\n",
        "458 2\n",
        "459 2\n",
        "460 1\n",
        "461 2\n",
        "462 6\n",
        "463 1\n",
        "464 1\n",
        "465 1\n",
        "466 7\n",
        "467 1\n",
        "468 2\n",
        "469 4\n",
        "470 1\n",
        "471 4\n",
        "472 1\n",
        "473 3\n",
        "474 1\n",
        "475 1\n",
        "476 2\n",
        "477 2\n",
        "478 4\n",
        "479 1\n",
        "480 3\n",
        "481 2\n",
        "482 2\n",
        "483 4\n",
        "484 2\n",
        "485 7\n",
        "486 1\n",
        "487 1\n",
        "488 4\n",
        "489 1\n",
        "490 4\n",
        "491 4\n",
        "492 1\n",
        "493 1\n",
        "494 1\n",
        "495 7\n",
        "496 7\n",
        "497 4\n",
        "498 1\n",
        "499 1\n",
        "500 7\n",
        "501 1\n",
        "502 2\n",
        "503 2\n",
        "504 5\n",
        "505 7\n",
        "506 4\n",
        "507 1\n",
        "508 1\n",
        "509 1\n",
        "510 4\n",
        "511 2\n",
        "512 3\n",
        "513 1\n",
        "514 2\n",
        "515 1\n",
        "516 1\n",
        "517 4\n",
        "518 7\n",
        "519 3\n",
        "520 1\n",
        "521 3\n",
        "522 4\n",
        "523 3\n",
        "524 1\n",
        "525 2\n",
        "526 2\n",
        "527 7\n",
        "528 5\n",
        "529 2\n",
        "530 2\n",
        "531 2\n",
        "532 4\n",
        "533 5\n",
        "534 1\n",
        "535 3\n",
        "536 3\n",
        "537 3\n",
        "538 1\n",
        "539 5\n",
        "540 2\n",
        "541 2\n",
        "542 1\n",
        "543 1\n",
        "544 1\n",
        "545 3\n",
        "546 4\n",
        "547 5\n",
        "548 7\n",
        "549 2\n",
        "550 5\n",
        "551 1\n",
        "552 2\n",
        "553 3\n",
        "554 1\n",
        "555 4\n",
        "556 1\n",
        "557 1\n",
        "558 5\n",
        "559 7\n",
        "560 6\n",
        "561 1\n",
        "562 1\n",
        "563 2\n",
        "564 3\n",
        "565 5\n",
        "566 1\n",
        "567 4\n",
        "568 1\n",
        "569 1\n",
        "570 2\n",
        "571 2\n",
        "572 2\n",
        "573 7\n",
        "574 4\n",
        "575 1\n",
        "576 1\n",
        "577 2\n",
        "578 6\n",
        "579 2\n",
        "580 7\n",
        "581 4\n",
        "582 2\n",
        "583 2\n",
        "584 1\n",
        "585 6\n",
        "586 6\n",
        "587 2\n",
        "588 1\n",
        "589 1\n",
        "590 2\n",
        "591 2\n",
        "592 1\n",
        "593 3\n",
        "594 2\n",
        "595 1\n",
        "596 6\n",
        "597 5\n",
        "598 4\n",
        "599 6\n",
        "600 1\n",
        "601 3\n",
        "602 1\n",
        "603 1\n",
        "604 2\n",
        "605 1\n",
        "606 7\n",
        "607 1\n",
        "608 4\n",
        "609 4\n",
        "610 6\n",
        "611 1\n",
        "612 7\n",
        "613 1\n",
        "614 1\n",
        "615 3\n",
        "616 1\n",
        "617 3\n",
        "618 4\n",
        "619 2\n",
        "620 2\n",
        "621 1\n",
        "622 2\n",
        "623 2\n",
        "624 1\n",
        "625 1\n",
        "626 1\n",
        "627 2\n",
        "628 2\n",
        "629 1\n",
        "630 4\n",
        "631 5\n",
        "632 1\n",
        "633 4\n",
        "634 1\n",
        "635 1\n",
        "636 3\n",
        "637 1\n",
        "638 6\n",
        "639 4\n",
        "640 3\n",
        "641 3\n",
        "642 6\n",
        "643 1\n",
        "644 4\n",
        "645 3\n",
        "646 3\n",
        "647 1\n",
        "648 2\n",
        "649 2\n",
        "650 1\n",
        "651 4\n",
        "652 1\n",
        "653 3\n",
        "654 5\n",
        "655 3\n",
        "656 6\n",
        "657 2\n",
        "658 6\n",
        "659 7\n",
        "660 1\n",
        "661 3\n",
        "662 2\n",
        "663 7\n",
        "664 5\n",
        "665 4\n",
        "666 3\n",
        "667 3\n",
        "668 3\n",
        "669 1\n",
        "670 6\n",
        "671 1\n",
        "672 1\n",
        "673 1\n",
        "674 3\n",
        "675 6\n",
        "676 1\n",
        "677 2\n",
        "678 1\n",
        "679 1\n",
        "680 1\n",
        "681 4\n",
        "682 1\n",
        "683 3\n",
        "684 4\n",
        "685 1\n",
        "686 2\n",
        "687 6\n",
        "688 7\n",
        "689 7\n",
        "690 6\n",
        "691 1\n",
        "692 1\n",
        "693 1\n",
        "694 2\n",
        "695 2\n",
        "696 5\n",
        "697 1\n",
        "698 4\n",
        "699 5\n",
        "700 3\n",
        "701 1\n",
        "702 2\n",
        "703 5\n",
        "704 1\n",
        "705 1\n",
        "706 2\n",
        "707 2\n",
        "708 5\n",
        "709 7\n",
        "710 4\n",
        "711 1\n",
        "712 6\n",
        "713 4\n",
        "714 1\n",
        "715 7\n",
        "716 2\n",
        "717 2\n",
        "718 1\n",
        "719 1\n",
        "720 6\n",
        "721 4\n",
        "722 1\n",
        "723 1\n",
        "724 7\n",
        "725 2\n",
        "726 1\n",
        "1734 1\n",
        "728 3\n",
        "729 7\n",
        "730 1\n",
        "731 1\n",
        "732 4\n",
        "733 1\n",
        "734 1\n",
        "735 2\n",
        "736 7\n",
        "737 4\n",
        "738 5\n",
        "739 5\n",
        "740 1\n",
        "741 4\n",
        "742 1\n",
        "743 2\n",
        "744 1\n",
        "745 1\n",
        "746 3\n",
        "747 5\n",
        "748 2\n",
        "749 4\n",
        "750 2\n",
        "751 1\n",
        "752 1\n",
        "753 3\n",
        "754 3\n",
        "755 5\n",
        "756 1\n",
        "757 1\n",
        "758 3\n",
        "759 2\n",
        "760 2\n",
        "761 1\n",
        "762 4\n",
        "763 1\n",
        "764 7\n",
        "765 1\n",
        "766 1\n",
        "767 1\n",
        "768 2\n",
        "769 1\n",
        "770 3\n",
        "771 6\n",
        "772 2\n",
        "773 4\n",
        "774 4\n",
        "775 4\n",
        "776 4\n",
        "777 3\n",
        "778 4\n",
        "779 7\n",
        "780 4\n",
        "781 3\n",
        "782 7\n",
        "783 3\n",
        "784 3\n",
        "785 1\n",
        "786 2\n",
        "787 6\n",
        "788 2\n",
        "789 1\n",
        "790 2\n",
        "791 1\n",
        "792 1\n",
        "793 7\n",
        "794 2\n",
        "795 2\n",
        "796 6\n",
        "797 1\n",
        "798 1\n",
        "799 2\n",
        "800 1\n",
        "801 1\n",
        "802 2\n",
        "803 2\n",
        "804 6\n",
        "805 4\n",
        "806 3\n",
        "807 2\n",
        "808 3\n",
        "809 3\n",
        "810 1\n",
        "811 5\n",
        "812 1\n",
        "813 3\n",
        "814 2\n",
        "815 4\n",
        "816 2\n",
        "817 7\n",
        "818 1\n",
        "819 5\n",
        "820 1\n",
        "821 4\n",
        "822 1\n",
        "823 4\n",
        "824 5\n",
        "825 2\n",
        "826 2\n",
        "827 3\n",
        "828 2\n",
        "829 7\n",
        "830 1\n",
        "831 1\n",
        "832 3\n",
        "833 4\n",
        "834 1\n",
        "835 5\n",
        "836 3\n",
        "837 2\n",
        "838 7\n",
        "839 1\n",
        "840 4\n",
        "841 1\n",
        "842 3\n",
        "843 4\n",
        "844 1\n",
        "845 1\n",
        "846 1\n",
        "847 2\n",
        "848 1\n",
        "849 1\n",
        "850 4\n",
        "851 4\n",
        "852 1\n",
        "853 1\n",
        "854 1\n",
        "855 3\n",
        "856 1\n",
        "857 3\n",
        "858 4\n",
        "859 1\n",
        "860 2\n",
        "861 7\n",
        "862 4\n",
        "863 1\n",
        "864 1\n",
        "865 1\n",
        "866 2\n",
        "867 4\n",
        "868 7\n",
        "869 6\n",
        "870 2\n",
        "871 3\n",
        "872 6\n",
        "953 7\n",
        "874 2\n",
        "875 5\n",
        "876 1\n",
        "877 1\n",
        "878 2\n",
        "879 1\n",
        "880 1\n",
        "881 1\n",
        "882 1\n",
        "883 5\n",
        "884 2\n",
        "885 4\n",
        "886 6\n",
        "887 1\n",
        "888 2\n",
        "889 6\n",
        "890 7\n",
        "891 3\n",
        "892 1\n",
        "893 1\n",
        "894 2\n",
        "895 4\n",
        "896 7\n",
        "897 1\n",
        "898 4\n",
        "899 3\n",
        "900 2\n",
        "901 1\n",
        "902 1\n",
        "903 2\n",
        "904 1\n",
        "905 5\n",
        "906 1\n",
        "907 5\n",
        "908 6\n",
        "909 6\n",
        "910 1\n",
        "911 4\n",
        "912 2\n",
        "913 7\n",
        "914 1\n",
        "915 4\n",
        "916 2\n",
        "917 6\n",
        "918 1\n",
        "919 1\n",
        "920 1\n",
        "921 1\n",
        "922 1\n",
        "923 1\n",
        "924 1\n",
        "925 1\n",
        "926 1\n",
        "927 4\n",
        "928 4\n",
        "929 5\n",
        "930 1\n",
        "931 5\n",
        "932 3\n",
        "933 3\n",
        "934 1\n",
        "935 2\n",
        "936 2\n",
        "937 1\n",
        "938 7\n",
        "939 3\n",
        "940 1\n",
        "941 1\n",
        "942 4\n",
        "943 4\n",
        "944 3\n",
        "945 4\n",
        "946 2\n",
        "947 6\n",
        "948 1\n",
        "949 5\n",
        "950 2\n",
        "951 1\n",
        "952 4\n",
        "954 1\n",
        "955 6\n",
        "956 1\n",
        "957 1\n",
        "958 1\n",
        "959 4\n",
        "960 1\n",
        "961 2\n",
        "962 1\n",
        "963 1\n",
        "964 1\n",
        "965 4\n",
        "966 2\n",
        "967 1\n",
        "968 2\n",
        "969 3\n",
        "970 1\n",
        "971 3\n",
        "972 1\n",
        "973 6\n",
        "974 2\n",
        "975 5\n",
        "976 1\n",
        "977 1\n",
        "978 2\n",
        "979 1\n",
        "980 1\n",
        "981 1\n",
        "982 4\n",
        "983 6\n",
        "984 4\n",
        "985 2\n",
        "986 1\n",
        "987 5\n",
        "988 1\n",
        "989 2\n",
        "990 3\n",
        "991 4\n",
        "992 1\n",
        "993 3\n",
        "994 3\n",
        "995 1\n",
        "996 3\n",
        "997 2\n",
        "998 1\n",
        "999 4\n",
        "1000 2\n",
        "1001 4\n",
        "1002 3\n",
        "1003 4\n",
        "1004 1\n",
        "1005 2\n",
        "1006 1\n",
        "1007 7\n",
        "1008 1\n",
        "1009 1\n",
        "1010 4\n",
        "1011 4\n",
        "1012 2\n",
        "1013 1\n",
        "1014 1\n",
        "1015 1\n",
        "1016 4\n",
        "1017 1\n",
        "1018 1\n",
        "1019 2\n",
        "1020 3\n",
        "1021 5\n",
        "1022 6\n",
        "1023 5\n",
        "1024 1\n",
        "1025 1\n",
        "1026 2\n",
        "1027 1\n",
        "1028 1\n",
        "1029 4\n",
        "1030 1\n",
        "1031 2\n",
        "1032 1\n",
        "1033 1\n",
        "1034 1\n",
        "1035 4\n",
        "1036 6\n",
        "1037 2\n",
        "1038 1\n",
        "1039 1\n",
        "1040 4\n",
        "1041 5\n",
        "1042 1\n",
        "1043 2\n",
        "1044 3\n",
        "1045 3\n",
        "1046 1\n",
        "1047 1\n",
        "1048 1\n",
        "1049 4\n",
        "1050 4\n",
        "1051 1\n",
        "1052 2\n",
        "1053 2\n",
        "1054 3\n",
        "1055 4\n",
        "1056 3\n",
        "1057 4\n",
        "1058 5\n",
        "1059 3\n",
        "1060 2\n",
        "1061 2\n",
        "1062 1\n",
        "1063 2\n",
        "1064 1\n",
        "1065 7\n",
        "1066 7\n",
        "1067 1\n",
        "1068 1\n",
        "1069 6\n",
        "1070 2\n",
        "1071 3\n",
        "1072 2\n",
        "1073 1\n",
        "1074 1\n",
        "1075 2\n",
        "1076 6\n",
        "1077 2\n",
        "1078 2\n",
        "1079 3\n",
        "1080 1\n",
        "1081 1\n",
        "1082 4\n",
        "1083 7\n",
        "1084 1\n",
        "1085 5\n",
        "1086 3\n",
        "1087 7\n",
        "1088 6\n",
        "1089 7\n",
        "1090 6\n",
        "1091 1\n",
        "1092 5\n",
        "1093 1\n",
        "1094 6\n",
        "1095 6\n",
        "1096 1\n",
        "1097 1\n",
        "1098 2\n",
        "1099 1\n",
        "1100 2\n",
        "1101 1\n",
        "1102 4\n",
        "1103 1\n",
        "1104 1\n",
        "1105 2\n",
        "1106 2\n",
        "1107 1\n",
        "1108 3\n",
        "1109 1\n",
        "1110 2\n",
        "1111 6\n",
        "1112 3\n",
        "1113 1\n",
        "1114 2\n",
        "1115 2\n",
        "1116 1\n",
        "1117 1\n",
        "1118 1\n",
        "1119 1\n",
        "1120 1\n",
        "1121 2\n",
        "1122 4\n",
        "1123 1\n",
        "1124 5\n",
        "1125 1\n",
        "1126 4\n",
        "1127 1\n",
        "1128 2\n",
        "1129 4\n",
        "1130 6\n",
        "1131 6\n",
        "1132 2\n",
        "1133 3\n",
        "1134 1\n",
        "1135 1\n",
        "1136 1\n",
        "1137 2\n",
        "1138 1\n",
        "1139 2\n",
        "1140 1\n",
        "1141 1\n",
        "1142 1\n",
        "1143 4\n",
        "1144 7\n",
        "1145 1\n",
        "1146 4\n",
        "1147 2\n",
        "1148 4\n",
        "1149 2\n",
        "1150 1\n",
        "1151 4\n",
        "1152 1\n",
        "1153 6\n",
        "1154 2\n",
        "1155 4\n",
        "1156 1\n",
        "1157 1\n",
        "1158 1\n",
        "1159 3\n",
        "1160 1\n",
        "1161 4\n",
        "1162 1\n",
        "1163 3\n",
        "1164 7\n",
        "1165 1\n",
        "1166 1\n",
        "1167 4\n",
        "1168 1\n",
        "1169 1\n",
        "1170 2\n",
        "1171 1\n",
        "1172 4\n",
        "1173 6\n",
        "1174 3\n",
        "1175 6\n",
        "1176 7\n",
        "1177 2\n",
        "1178 1\n",
        "1179 1\n",
        "1180 6\n",
        "1181 4\n",
        "1182 6\n",
        "1183 2\n",
        "1184 2\n",
        "1185 1\n",
        "1186 4\n",
        "1187 3\n",
        "1188 4\n",
        "1189 1\n",
        "1190 1\n",
        "1191 1\n",
        "1192 5\n",
        "1193 1\n",
        "1194 1\n",
        "1195 7\n",
        "1196 6\n",
        "1197 6\n",
        "1198 1\n",
        "1199 2\n",
        "1200 4\n",
        "1201 4\n",
        "1202 1\n",
        "1203 5\n",
        "1204 6\n",
        "1205 1\n",
        "1206 2\n",
        "1207 3\n",
        "1208 2\n",
        "1209 6\n",
        "1210 4\n",
        "1211 1\n",
        "1212 1\n",
        "1213 3\n",
        "1214 4\n",
        "1215 1\n",
        "1216 3\n",
        "1217 1\n",
        "1218 1\n",
        "1219 6\n",
        "1220 5\n",
        "1221 4\n",
        "1222 3\n",
        "1223 1\n",
        "1224 2\n",
        "1225 1\n",
        "1226 4\n",
        "1227 2\n",
        "1228 4\n",
        "1229 1\n",
        "1230 2\n",
        "1231 1\n",
        "1232 1\n",
        "1233 2\n",
        "1234 4\n",
        "1235 4\n",
        "1236 2\n",
        "1237 1\n",
        "1238 1\n",
        "1239 1\n",
        "1240 7\n",
        "1241 4\n",
        "1242 6\n",
        "1243 7\n",
        "1244 1\n",
        "1245 3\n",
        "1246 2\n",
        "1247 2\n",
        "1248 1\n",
        "1249 2\n",
        "1250 4\n",
        "1251 3\n",
        "1252 1\n",
        "1253 5\n",
        "1254 4\n",
        "1255 1\n",
        "1256 1\n",
        "1257 1\n",
        "1258 4\n",
        "1259 1\n",
        "1260 1\n",
        "1261 5\n",
        "1262 7\n",
        "1263 1\n",
        "1264 6\n",
        "1265 2\n",
        "1266 2\n",
        "1267 2\n",
        "1268 1\n",
        "1269 1\n",
        "1270 1\n",
        "1271 7\n",
        "1272 1\n",
        "1273 4\n",
        "1274 6\n",
        "1275 1\n",
        "1276 4\n",
        "1277 3\n",
        "1278 4\n",
        "1279 4\n",
        "1280 6\n",
        "1281 2\n",
        "1282 6\n",
        "1283 7\n",
        "1284 3\n",
        "1285 2\n",
        "1286 2\n",
        "1287 2\n",
        "1288 1\n",
        "1289 3\n",
        "1290 3\n",
        "1291 2\n",
        "1292 1\n",
        "1293 2\n",
        "1294 1\n",
        "1295 1\n",
        "1296 1\n",
        "1297 6\n",
        "1298 2\n",
        "1299 7\n",
        "1300 3\n",
        "1301 3\n",
        "1302 2\n",
        "1303 6\n",
        "1304 3\n",
        "1305 1\n",
        "1306 1\n",
        "1307 1\n",
        "1308 4\n",
        "1309 2\n",
        "1310 1\n",
        "1311 1\n",
        "1312 3\n",
        "1313 4\n",
        "1314 1\n",
        "1315 2\n",
        "1316 3\n",
        "1317 2\n",
        "1318 3\n",
        "1319 1\n",
        "1320 2\n",
        "1321 1\n",
        "1322 2\n",
        "1323 1\n",
        "1324 1\n",
        "1325 1\n",
        "1326 6\n",
        "1327 1\n",
        "1328 6\n",
        "1329 5\n",
        "1330 1\n",
        "1331 4\n",
        "1332 3\n",
        "1333 6\n",
        "1334 5\n",
        "1335 1\n",
        "1336 4\n",
        "1337 1\n",
        "1338 5\n",
        "1339 4\n",
        "1340 1\n",
        "1341 1\n",
        "1342 1\n",
        "1343 1\n",
        "1344 2\n",
        "1345 1\n",
        "1346 4\n",
        "1347 2\n",
        "1348 1\n",
        "1349 2\n",
        "1350 4\n",
        "1351 2\n",
        "1352 3\n",
        "1353 4\n",
        "1354 2\n",
        "1355 1\n",
        "1356 1\n",
        "1357 3\n",
        "1358 7\n",
        "1359 3\n",
        "1360 4\n",
        "1361 1\n",
        "1362 4\n",
        "1363 2\n",
        "1364 4\n",
        "1365 2\n",
        "1366 1\n",
        "1367 2\n",
        "1368 1\n",
        "1369 1\n",
        "1370 6\n",
        "1371 7\n",
        "1372 1\n",
        "1373 1\n",
        "1374 1\n",
        "1375 2\n",
        "1376 7\n",
        "1377 1\n",
        "1378 1\n",
        "1379 1\n",
        "1380 3\n",
        "1381 4\n",
        "1382 7\n",
        "1383 2\n",
        "1384 3\n",
        "1385 3\n",
        "1386 1\n",
        "1387 2\n",
        "1388 2\n",
        "1389 1\n",
        "1390 2\n",
        "1391 4\n",
        "1392 1\n",
        "1393 2\n",
        "1394 1\n",
        "1395 1\n",
        "1396 1\n",
        "1397 1\n",
        "1398 7\n",
        "1399 4\n",
        "1400 1\n",
        "1401 1\n",
        "1402 1\n",
        "1403 3\n",
        "1404 3\n",
        "1405 2\n",
        "1406 3\n",
        "1407 4\n",
        "1408 1\n",
        "1409 5\n",
        "1410 3\n",
        "1411 6\n",
        "1412 2\n",
        "1413 4\n",
        "1414 3\n",
        "1415 4\n",
        "1416 4\n",
        "1417 1\n",
        "1418 1\n",
        "1419 1\n",
        "1420 1\n",
        "1421 1\n",
        "1422 4\n",
        "1423 1\n",
        "1424 1\n",
        "1425 1\n",
        "1426 4\n",
        "1427 2\n",
        "1428 1\n",
        "1429 1\n",
        "1430 3\n",
        "1431 1\n",
        "1432 6\n",
        "1433 3\n",
        "1434 4\n",
        "1435 6\n",
        "1436 1\n",
        "1437 3\n",
        "1438 2\n",
        "1439 3\n",
        "1440 2\n",
        "1441 1\n",
        "1442 4\n",
        "1443 1\n",
        "1444 2\n",
        "1445 5\n",
        "1446 2\n",
        "1447 1\n",
        "1448 1\n",
        "1449 3\n",
        "1450 1\n",
        "1451 1\n",
        "1452 3\n",
        "1453 1\n",
        "1454 5\n",
        "1455 4\n",
        "1456 1\n",
        "1457 4\n",
        "1458 3\n",
        "1459 4\n",
        "1460 1\n",
        "1461 1\n",
        "1462 3\n",
        "1463 2\n",
        "1464 1\n",
        "1465 5\n",
        "1466 2\n",
        "1467 4\n",
        "1468 2\n",
        "1469 6\n",
        "1470 2\n",
        "1471 3\n",
        "1472 1\n",
        "1473 1\n",
        "1474 1\n",
        "1475 2\n",
        "1476 1\n",
        "1477 1\n",
        "1478 4\n",
        "1479 2\n",
        "1480 6\n",
        "1481 4\n",
        "1482 3\n",
        "1483 2\n",
        "1484 1\n",
        "1485 3\n",
        "1486 1\n",
        "1487 7\n",
        "1488 2\n",
        "1489 1\n",
        "1490 2\n",
        "1491 5\n",
        "1492 5\n",
        "1493 1\n",
        "1494 1\n",
        "1495 6\n",
        "1496 1\n",
        "1497 1\n",
        "1498 1\n",
        "1499 4\n",
        "1500 3\n",
        "1501 1\n",
        "1502 2\n",
        "1503 5\n",
        "1504 2\n",
        "1505 5\n",
        "1506 2\n",
        "1507 1\n",
        "1508 3\n",
        "1509 1\n",
        "1510 1\n",
        "1511 3\n",
        "1512 1\n",
        "1513 1\n",
        "1514 2\n",
        "1515 3\n",
        "1516 5\n",
        "1517 1\n",
        "1518 1\n",
        "1519 1\n",
        "1520 1\n",
        "1521 1\n",
        "1522 2\n",
        "1523 1\n",
        "1524 2\n",
        "1525 3\n",
        "1526 1\n",
        "1527 1\n",
        "1528 5\n",
        "1529 4\n",
        "1530 5\n",
        "1531 6\n",
        "1532 2\n",
        "1533 4\n",
        "1534 2\n",
        "1535 3\n",
        "1536 4\n",
        "1537 3\n",
        "1538 3\n",
        "1539 7\n",
        "1540 2\n",
        "1541 1\n",
        "1542 3\n",
        "1543 1\n",
        "1544 2\n",
        "1545 3\n",
        "1546 3\n",
        "1547 1\n",
        "1548 5\n",
        "1549 2\n",
        "1550 1\n",
        "1551 1\n",
        "1552 2\n",
        "1553 7\n",
        "1554 1\n",
        "1555 4\n",
        "1556 4\n",
        "1557 4\n",
        "1558 2\n",
        "1559 5\n",
        "1560 4\n",
        "1561 1\n",
        "1562 2\n",
        "1563 4\n",
        "1564 1\n",
        "1565 4\n",
        "1566 1\n",
        "1567 6\n",
        "1568 1\n",
        "1569 1\n",
        "1570 3\n",
        "1571 6\n",
        "1572 3\n",
        "1573 4\n",
        "1574 4\n",
        "1575 2\n",
        "1576 7\n",
        "1577 1\n",
        "1578 4\n",
        "1579 4\n",
        "1580 6\n",
        "1581 7\n",
        "1582 3\n",
        "1583 2\n",
        "1584 2\n",
        "1585 4\n",
        "1586 3\n",
        "1587 5\n",
        "1588 1\n",
        "1589 1\n",
        "1590 5\n",
        "1591 4\n",
        "1592 2\n",
        "1593 1\n",
        "1594 2\n",
        "1595 3\n",
        "1596 7\n",
        "1597 1\n",
        "1598 3\n",
        "1599 2\n",
        "1600 1\n",
        "1601 5\n",
        "1602 4\n",
        "1603 3\n",
        "1604 6\n",
        "1605 1\n",
        "1606 2\n",
        "1607 1\n",
        "1608 2\n",
        "1609 7\n",
        "1610 3\n",
        "1611 4\n",
        "1612 1\n",
        "1613 1\n",
        "1614 2\n",
        "1615 4\n",
        "1616 1\n",
        "1617 4\n",
        "1618 1\n",
        "1619 5\n",
        "1620 4\n",
        "1621 7\n",
        "1622 2\n",
        "1623 3\n",
        "1624 1\n",
        "1625 2\n",
        "1626 1\n",
        "1627 7\n",
        "1628 1\n",
        "1629 1\n",
        "1630 2\n",
        "1631 3\n",
        "1632 6\n",
        "1633 2\n",
        "1634 1\n",
        "1635 3\n",
        "1636 5\n",
        "1637 1\n",
        "1638 6\n",
        "1639 4\n",
        "1640 1\n",
        "1641 2\n",
        "1642 2\n",
        "1643 4\n",
        "1644 2\n",
        "1645 3\n",
        "1646 1\n",
        "1647 1\n",
        "1648 1\n",
        "1649 1\n",
        "1650 1\n",
        "1651 1\n",
        "1652 1\n",
        "1653 4\n",
        "1654 2\n",
        "1655 2\n",
        "1656 1\n",
        "1657 3\n",
        "1658 2\n",
        "1659 5\n",
        "1660 6\n",
        "1661 1\n",
        "1662 2\n",
        "1663 4\n",
        "1664 1\n",
        "1665 1\n",
        "1666 1\n",
        "1667 1\n",
        "1668 5\n",
        "1669 4\n",
        "1670 3\n",
        "1671 1\n",
        "1672 3\n",
        "1673 1\n",
        "1674 4\n",
        "1675 2\n",
        "1676 4\n",
        "1677 1\n",
        "1678 7\n",
        "1679 2\n",
        "1680 1\n",
        "1681 6\n",
        "1682 1\n",
        "1683 2\n",
        "1684 2\n",
        "1685 1\n",
        "1686 2\n",
        "1687 3\n",
        "1688 4\n",
        "1689 1\n",
        "1690 1\n",
        "1691 2\n",
        "1692 5\n",
        "1693 6\n",
        "1694 3\n",
        "1695 4\n",
        "1696 1\n",
        "1697 1\n",
        "1698 4\n",
        "1699 1\n",
        "1700 1\n",
        "1701 4\n",
        "1702 5\n",
        "1703 3\n",
        "1704 3\n",
        "1705 2\n",
        "1706 2\n",
        "1707 3\n",
        "1708 4\n",
        "1709 2\n",
        "1710 1\n",
        "1711 6\n",
        "1712 2\n",
        "1713 2\n",
        "1714 4\n",
        "1715 1\n",
        "1716 3\n",
        "1717 2\n",
        "1718 2\n",
        "1719 3\n",
        "1720 1\n",
        "1721 1\n",
        "1722 1\n",
        "1723 3\n",
        "1724 6\n",
        "1725 1\n",
        "1726 2\n",
        "1727 2\n",
        "1728 1\n",
        "1729 4\n",
        "1730 3\n",
        "1731 3\n",
        "1732 7\n",
        "1733 1\n",
        "1735 4\n",
        "1736 1\n",
        "1737 4\n",
        "1738 1\n",
        "1739 4\n",
        "1740 3\n",
        "1741 3\n",
        "1742 1\n",
        "1743 1\n",
        "1744 3\n",
        "1745 1\n",
        "1746 1\n",
        "1747 5\n",
        "1748 4\n",
        "1749 1\n",
        "1750 7\n",
        "1751 2\n",
        "1752 3\n",
        "1753 1\n",
        "1754 4\n",
        "1755 4\n",
        "1756 3\n",
        "1757 1\n",
        "1758 1\n",
        "1759 1\n",
        "1760 4\n",
        "1761 4\n",
        "1762 2\n",
        "1763 1\n",
        "1764 6\n",
        "1765 3\n",
        "1766 2\n",
        "1767 4\n",
        "1768 5\n",
        "1769 4\n",
        "1770 5\n",
        "1771 6\n",
        "1772 1\n",
        "1773 1\n",
        "1774 5\n",
        "1775 4\n",
        "1776 4\n",
        "1777 1\n",
        "1778 3\n",
        "1779 2\n",
        "1780 4\n",
        "1781 3\n",
        "1782 5\n",
        "1783 6\n",
        "1784 6\n",
        "1785 2\n",
        "1786 5\n",
        "1787 1\n",
        "1788 3\n",
        "1789 1\n",
        "1790 2\n",
        "1791 4\n",
        "1792 3\n",
        "1793 2\n",
        "1794 4\n",
        "1795 1\n",
        "1796 2\n",
        "1797 3\n",
        "1798 4\n",
        "1799 5\n",
        "1800 4\n",
        "1801 1\n",
        "1802 1\n",
        "1803 1\n",
        "1804 4\n",
        "1805 1\n",
        "1806 6\n",
        "1807 4\n",
        "1808 4\n",
        "1809 1\n",
        "1810 1\n",
        "1811 2\n",
        "1812 2\n",
        "1813 2\n",
        "1814 3\n",
        "1815 2\n",
        "1816 1\n",
        "1817 1\n",
        "1818 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3\n",
        "1819 2\n",
        "1820 7\n",
        "1821 4\n",
        "1822 1\n",
        "1823 5\n",
        "1824 4\n",
        "1825 4\n",
        "1826 1\n",
        "1827 1\n",
        "1828 1\n",
        "1829 2\n",
        "1830 3\n",
        "1831 2\n",
        "1832 2\n",
        "1833 1\n",
        "1834 4\n",
        "1835 3\n",
        "1836 2\n",
        "1837 1\n",
        "1838 4\n",
        "1839 4\n",
        "1840 1\n",
        "1841 2\n",
        "1842 7\n",
        "1843 1\n",
        "1844 1\n",
        "1845 4\n",
        "1846 1\n",
        "1847 4\n",
        "1848 2\n",
        "1849 1\n",
        "1850 1\n",
        "1851 4\n",
        "1852 7\n",
        "1853 2\n",
        "1854 2\n",
        "1855 2\n",
        "1856 2\n",
        "1857 1\n",
        "1858 1\n",
        "1859 4\n",
        "1860 7\n",
        "1861 4\n",
        "1862 1\n",
        "1863 7\n",
        "1864 3\n",
        "1865 1\n",
        "1866 1\n",
        "1867 4\n",
        "1868 1\n",
        "1869 2\n",
        "1870 2\n",
        "1871 1\n",
        "1872 7\n",
        "1873 3\n",
        "1874 4\n",
        " 1\n"
       ]
      }
     ],
     "prompt_number": 253
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#function to throw out the most occuring and least occuring words;\n",
      "#do this by term frequency for a word, and then figuring out the overall tf distribution \n",
      "#of words in the collection\n",
      "def get_ngram_cutoff(ngram, lowerbound, upperbound):\n",
      "  fdist_ngram = nltk.FreqDist(ngram)\n",
      "  fdist_keys = fdist_ngram.keys()\n",
      "  fdist_vals =  fdist_ngram.values()\n",
      "  percentilescore = [stats.percentileofscore(fdist_vals, i) for i in fdist_vals]\n",
      "  new_fdist = dict(zip(fdist_keys, percentilescore))\n",
      "  #get the keywords:\n",
      "  keywords = []  \n",
      "  new_key_word_dict = []\n",
      "  #if the term freq of a word is between the xth and yth percentile, keep it; if not, throw it out. \n",
      "  for word, percentile in new_fdist.iteritems():\n",
      "      if percentile > lowerbound and percentile < upperbound:\n",
      "         #get the words freq count\n",
      "        #new_key_word_dict[word[0]] = fdist_ngram[word]\n",
      "        new_key_word_dict.append( (word) + (fdist_ngram[word],))\n",
      "  sorted_keywords = sorted_key_word_dict = sorted(new_key_word_dict,  key=lambda tup: tup[2], reverse = True)\n",
      "  return sorted_keywords "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 244
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#andidates, key=NWORDS.get)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}