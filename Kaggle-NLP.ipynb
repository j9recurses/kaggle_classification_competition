{
 "metadata": {
  "name": "",
  "signature": "sha256:b966b6460bb8b42f3979d77d4fee5c71f1037e22ee6e2d17928ada4a21937d9d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk \n",
      "import random\n",
      "from random import shuffle \n",
      "from collections import Counter\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "from nltk import word_tokenize  \n",
      "import nltk.tag, nltk.data\n",
      "from nltk.corpus import wordnet\n",
      "from nltk.stem.porter import PorterStemmer\n",
      "import numpy as np\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn import metrics\n",
      "from operator import itemgetter\n",
      "from sklearn.metrics import classification_report\n",
      "import csv\n",
      "import os\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Class to pre-process the collection\n",
      "#class is full of goodies/functions for preprocessing data- remove stopwords, punk, and convert to lower, lemmatize \n",
      "class PreprocessText:\n",
      "\n",
      "    #function to remove punct. \n",
      "    def remove_punct(self, text):\n",
      "      exclude = set(string.punctuation)\n",
      "      table = string.maketrans(\"\",\"\")\n",
      "      text = text.translate(table, string.punctuation)\n",
      "      return text\n",
      "\n",
      "    #remove stopwords-> A quick way to reduce elminate words that aren't valid key words.\n",
      "    def removestopwords(self, tokens):\n",
      "      stopwords = nltk.corpus.stopwords.words('english')\n",
      "      tokens = [w for w in tokens if w.lower().strip() not in stopwords]\n",
      "      return tokens\n",
      "\n",
      "    ##lemmatize the words to reduce dimensionality. Also,option to do lemmatization based on POS. \n",
      "    #wordnet lemmatizer assumes everything is a noun unless otherwise specified, so we need to give\n",
      "    #it the wordnet pos if we don't want the default noun lookup. \n",
      "    def lemmatize(self, tokens, lemmatize_pos):\n",
      "        def get_wordnet_pos( pos_tag):\n",
      "            if pos_tag[1].startswith('J'):\n",
      "                return (pos_tag[0], wordnet.ADJ)\n",
      "            elif pos_tag[1].startswith('V'):\n",
      "                return (pos_tag[0], wordnet.VERB)\n",
      "            elif pos_tag[1].startswith('N'):\n",
      "                return (pos_tag[0], wordnet.NOUN)\n",
      "            elif pos_tag[1].startswith('R'):\n",
      "                return (pos_tag[0], wordnet.ADV)\n",
      "            else:\n",
      "                return (pos_tag[0], wordnet.NOUN)\n",
      "        lemmatizer = WordNetLemmatizer()\n",
      "        if lemmatize_pos:\n",
      "            tokens_pos = nltk.tag.pos_tag(tokens)\n",
      "            tokens_pos_wordnet = [ get_wordnet_pos(token) for token in tokens_pos]\n",
      "            tokens_lemm = [lemmatizer.lemmatize(token[0], token[1]) for token in tokens_pos_wordnet]\n",
      "        else:\n",
      "            tokens_lemm = [lemmatizer.lemmatize(token) for token in tokens] \n",
      "        return tokens_lemm\n",
      "    \n",
      "    #function that combines above functions in one routine\n",
      "    #lots of args to specify what preprocessing routine you want to use\n",
      "    def preprocess_txt(self, text, convertlower=True, nopunk=True, stopwords=True, lemmatize_doc=True, lemmatize_pos=True, stemmed=False):\n",
      "      #convert to lower\n",
      "      if convertlower:\n",
      "        text = text.lower()\n",
      "      # remove punctuation\n",
      "      if nopunk:\n",
      "        text = remove_punct(text)\n",
      "      #tokenize text\n",
      "      tokens = PunktWordTokenizer().tokenize(text)\n",
      "      #remove stopwords\n",
      "      if stopwords:\n",
      "        tokens = removestopwords(tokens)\n",
      "      #lemmatize\n",
      "      if lemmatize_doc:\n",
      "        tokens = self.lemmatize(tokens,lemmatize_pos)\n",
      "      #stem\n",
      "        porter = PorterStemmer()\n",
      "        tokens = [ porter.stem(token) for token in tokens ]\n",
      "      return tokens\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#function to break up the data into training \n",
      "def prep_data():\n",
      "    f = open('data/train.txt','rb')\n",
      "    train_raw = f.read()\n",
      "    f.close() \n",
      "    \n",
      "    train_split = train_raw.split('\\n')\n",
      "    train_tuples = [ (line[2:], line[0]) for line in train_split if line != '']\n",
      "    shuffle(train_tuples)\n",
      "    total_size = len(train_tuples)\n",
      "    train_size = int(total_size * 0.9) \n",
      "    \n",
      "    yahoo_train_words = [ words for (words, cat) in train_tuples[:train_size] ]\n",
      "    yahoo_train_cats = [ cat for (line, cat) in train_tuples[:train_size] ]\n",
      "    yahoo_test_words = [ words for (words, cat) in train_tuples[train_size:] ]\n",
      "    yahoo_test_cats = [ cat for (words, cat) in train_tuples[train_size:] ]\n",
      "    \n",
      "    return np.array(yahoo_train_words), np.array(yahoo_train_cats), np.array(yahoo_test_words), np.array(yahoo_test_cats)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get the test and training sets, print the size\n",
      "train_words, train_cats, test_words, test_cats = prep_data()\n",
      "print 'The training set size is: ' + str(len(train_words))\n",
      "print 'The test set size is: ' + str(len(test_words))  \n",
      "print \"train_words[0] is: \" + str(train_words[0])\n",
      "print \"test_words[0] is: \" + str(test_words[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The training set size is: 2428\n",
        "The test set size is: 270\n",
        "train_words[0] is: where can i find places to download music videos for free? it seems like torrent sites hardly have any music videos, and plus itunes doesn't have everything yet.. i'm looking for music videos for bands like pink floyd, smashing pumpkins, radiohead etc.\n",
        "test_words[0] is: how do i find an out of print book? when i was a kid i remember seeing a book that was like an yearbook of all newspapers published by the times during ww ii. each of the years is compiled into a different book. &#xd;&lt;br&gt;it gave one a very uniqie perspecitev into the uk druing the war, and even had advertisements from thaat time.&#xd;&lt;br&gt;anybody out there know how to track such books?\n"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#pt = PreprocessText()\n",
      "#yahoo_train_pr = [( sent[1], pt.preprocess_txt(sent[0])) for sent in yahoo_train ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 133
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# #transform the training set into a dict, get the keys and vals for the tet and train\n",
      "# def get_train_and_test_dicts(yahoo_train, yahoo_test):\n",
      "#     yahoo_train_keys = dict(yahoo_train).keys()\n",
      "#     print yahoo_train_keys\n",
      "#     #yahoo_train_keys = [unicode(word) for word in yahoo_train_keys]\n",
      "#     yahoo_train_vals = dict(yahoo_train).values()\n",
      "#     yahoo_test_keys = dict(yahoo_test).keys()\n",
      "#     #yahoo_test_keys = [unicode(word) for word in yahoo_test_keys ]\n",
      "#     yahoo_test_vals = dict(yahoo_test).values()\n",
      "#     return np.array(yahoo_train_keys), np.array(yahoo_train_vals), np.array(yahoo_test_keys), np.array(yahoo_test_vals)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# train_words, train_cats, test_words, test_cats = get_train_and_test_dicts(yahoo_train, yahoo_test)\n",
      "#lol. Don't need to do preprocessing- Scipylearn can do all the preprocessing for you\n",
      "#want to use tfidf vectors to just grab the words that are most relevant\n",
      "#Convert a collection of raw documents to a matrix of TF-IDF features.\n",
      "#http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
      "\n",
      "\n",
      "\n",
      "class LemmaTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.wnl = WordNetLemmatizer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
      "    \n",
      "#vect = CountVectorizer(tokenizer=LemmaTokenizer()) \n",
      "\n",
      "#Convert a collection of raw documents to a matrix of TF-IDF features\n",
      "vectorizer = TfidfVectorizer(ngram_range=(1, 1), stop_words='english', strip_accents='unicode', tokenizer=LemmaTokenizer())\n",
      "\n",
      "#see what scipy is doing:\n",
      "test_string = unicode(train_words[0])\n",
      "\n",
      "print \"Example string: \" + test_string\n",
      "print \"Preprocessed string: \" + vectorizer.build_preprocessor()(test_string)\n",
      "print \"Tokenized string:\" + str(vectorizer.build_tokenizer()(test_string))\n",
      "print \"N-gram data string:\" + str(vectorizer.build_analyzer()(test_string))\n",
      "print \"\\n\"\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Example string: where can i find places to download music videos for free? it seems like torrent sites hardly have any music videos, and plus itunes doesn't have everything yet.. i'm looking for music videos for bands like pink floyd, smashing pumpkins, radiohead etc.\n",
        "Preprocessed string: where can i find places to download music videos for free? it seems like torrent sites hardly have any music videos, and plus itunes doesn't have everything yet.. i'm looking for music videos for bands like pink floyd, smashing pumpkins, radiohead etc.\n",
        "Tokenized string:[u'where', u'can', u'i', u'find', u'place', u'to', u'download', u'music', u'video', u'for', u'free', u'?', u'it', u'seems', u'like', u'torrent', u'site', u'hardly', u'have', u'any', u'music', u'video', u',', u'and', u'plus', u'itunes', u'doe', u\"n't\", u'have', u'everything', u'yet..', u'i', u\"'m\", u'looking', u'for', u'music', u'video', u'for', u'band', u'like', u'pink', u'floyd', u',', u'smashing', u'pumpkin', u',', u'radiohead', u'etc', u'.']\n",
        "N-gram data string:[u'place', u'download', u'music', u'video', u'free', u'?', u'like', u'torrent', u'site', u'hardly', u'music', u'video', u',', u'plus', u'itunes', u'doe', u\"n't\", u'yet..', u\"'m\", u'looking', u'music', u'video', u'band', u'like', u'pink', u'floyd', u',', u'smashing', u'pumpkin', u',', u'radiohead', u'.']\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Created a vocabulary from the training data (the fit in .fit_transform())\n",
      "#then turn the words into a TF-IDF weighted word vector(the transform in .fit_transform())\n",
      "\n",
      "X_train_words = vectorizer.fit_transform(train_words)\n",
      "\n",
      "#transformed the test data into a TF-IDF weighted word vector in the vocab space of the training data(.transform())\n",
      "X_test_words = vectorizer.transform(test_words)\n",
      "\n",
      "print \"X_train_words\"\n",
      "print X_train_words\n",
      "print \"X_test_words\"\n",
      "print X_test_words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "X_train_words\n",
        "  (0, 7160)\t0.207538101241\n",
        "  (0, 6823)\t0.453766039817\n",
        "  (0, 6522)\t0.179396723923\n",
        "  (0, 5909)\t0.207538101241\n",
        "  (0, 5857)\t0.124080702339\n",
        "  (0, 5237)\t0.207538101241\n",
        "  (0, 5181)\t0.207538101241\n",
        "  (0, 4949)\t0.179396723923\n",
        "  (0, 4914)\t0.138170351316\n",
        "  (0, 4903)\t0.19715195618\n",
        "  (0, 4371)\t0.0976363285729\n",
        "  (0, 4354)\t0.41199128209\n",
        "  (0, 3930)\t0.128359857121\n",
        "  (0, 3855)\t0.18869559099\n",
        "  (0, 3574)\t0.19715195618\n",
        "  (0, 3101)\t0.207538101241\n",
        "  (0, 2787)\t0.127221208086\n",
        "  (0, 2712)\t0.207538101241\n",
        "  (0, 2153)\t0.142840588283\n",
        "  (0, 2119)\t0.0871334845144\n",
        "  (0, 875)\t0.163870321634\n",
        "  (0, 376)\t0.0258484468862\n",
        "  (0, 71)\t0.0787653420112\n",
        "  (0, 59)\t0.192254917181\n",
        "  (0, 24)\t0.113368242221\n",
        "  :\t:\n",
        "  (2425, 376)\t0.0665509095766\n",
        "  (2425, 374)\t0.158382215897\n",
        "  (2425, 169)\t0.178113208248\n",
        "  (2425, 135)\t0.315939644245\n",
        "  (2425, 59)\t0.164996884407\n",
        "  (2425, 48)\t0.0831991955453\n",
        "  (2425, 47)\t0.0839444743329\n",
        "  (2425, 33)\t0.0736120937302\n",
        "  (2425, 24)\t0.0972948158179\n",
        "  (2425, 4)\t0.160637440355\n",
        "  (2425, 2)\t0.254988110788\n",
        "  (2425, 1)\t0.169746995739\n",
        "  (2426, 7151)\t0.499172264224\n",
        "  (2426, 6388)\t0.474191426894\n",
        "  (2426, 2007)\t0.499172264224\n",
        "  (2426, 376)\t0.0621708866068\n",
        "  (2426, 192)\t0.499172264224\n",
        "  (2426, 59)\t0.154137676798\n",
        "  (2427, 6934)\t0.285210017727\n",
        "  (2427, 5053)\t0.477126517414\n",
        "  (2427, 4390)\t0.454785248088\n",
        "  (2427, 3959)\t0.454785248088\n",
        "  (2427, 2954)\t0.284476324604\n",
        "  (2427, 1022)\t0.438098333934\n",
        "  (2427, 376)\t0.0670026787192\n",
        "X_test_words\n",
        "  (0, 7151)\t0.150804213653\n",
        "  (0, 7149)\t0.0787495232488\n",
        "  (0, 7125)\t0.250002214806\n",
        "  (0, 6902)\t0.125001107403\n",
        "  (0, 6876)\t0.145084483403\n",
        "  (0, 6544)\t0.137902660528\n",
        "  (0, 6460)\t0.137902660528\n",
        "  (0, 6459)\t0.0803690665929\n",
        "  (0, 5687)\t0.127486528111\n",
        "  (0, 5404)\t0.114584974985\n",
        "  (0, 5177)\t0.13374928811\n",
        "  (0, 5071)\t0.143257288874\n",
        "  (0, 4437)\t0.150804213653\n",
        "  (0, 3965)\t0.241695469969\n",
        "  (0, 3855)\t0.0685563037555\n",
        "  (0, 3714)\t0.0638288191875\n",
        "  (0, 3684)\t0.112099554278\n",
        "  (0, 3023)\t0.231928694337\n",
        "  (0, 2866)\t0.137902660528\n",
        "  (0, 2020)\t0.101030266889\n",
        "  (0, 1080)\t0.250002214806\n",
        "  (0, 1047)\t0.150804213653\n",
        "  (0, 1046)\t0.326702712877\n",
        "  (0, 654)\t0.10794618186\n",
        "  (0, 509)\t0.13374928811\n",
        "  :\t:\n",
        "  (265, 43)\t0.18907641591\n",
        "  (265, 33)\t0.125183142071\n",
        "  (266, 6118)\t0.334866081573\n",
        "  (266, 5809)\t0.483002831711\n",
        "  (266, 4004)\t0.35542361397\n",
        "  (266, 2902)\t0.315881585394\n",
        "  (266, 1744)\t0.359037727383\n",
        "  (266, 1248)\t0.330094366471\n",
        "  (266, 376)\t0.0601570167921\n",
        "  (266, 73)\t0.248026954673\n",
        "  (266, 71)\t0.183310356048\n",
        "  (266, 59)\t0.149144773664\n",
        "  (266, 24)\t0.263841587118\n",
        "  (267, 4387)\t0.704380435924\n",
        "  (267, 1560)\t0.704380435924\n",
        "  (267, 376)\t0.0877291455245\n",
        "  (268, 7084)\t0.622406887467\n",
        "  (268, 5463)\t0.776692928906\n",
        "  (268, 376)\t0.0967355189223\n",
        "  (269, 7136)\t0.510679349243\n",
        "  (269, 5033)\t0.503182727397\n",
        "  (269, 3017)\t0.490043088975\n",
        "  (269, 636)\t0.376592394247\n",
        "  (269, 376)\t0.0793706381576\n",
        "  (269, 0)\t0.312649942344\n"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#build the classifier\n",
      "bayes_classifier = MultinomialNB().fit(X_train_words, train_cats)\n",
      "\n",
      "yahoo_predicted = bayes_classifier.predict(X_test_words)\n",
      "\n",
      "print \"MODEL: Multinomial Naive Bayes\"\n",
      "\n",
      "print 'The precision for this classifier is ' + str(metrics.precision_score(test_cats, yahoo_predicted))\n",
      "print 'The recall for this classifier is ' + str(metrics.recall_score(test_cats, yahoo_predicted))\n",
      "print 'The f1 for this classifier is ' + str(metrics.f1_score(test_cats, yahoo_predicted))\n",
      "print 'The accuracy for this classifier is ' + str(metrics.accuracy_score(test_cats, yahoo_predicted))\n",
      "\n",
      "print '\\nHere is the classification report:'\n",
      "print classification_report(test_cats, yahoo_predicted)\n",
      "\n",
      "#simple thing to do would be to up the n-grams to bigrams; try varying ngram_range from (1, 1) to (1, 2)\n",
      "#we could also modify the vectorizer to stem or lemmatize\n",
      "print '\\nHere is the confusion matrix:'\n",
      "print metrics.confusion_matrix(test_cats, yahoo_predicted)\n",
      "\n",
      "##print the \n",
      "N = 10\n",
      "vocabulary = np.array([t for t, i in sorted(vectorizer.vocabulary_.iteritems(), key=itemgetter(1))])\n",
      "\n",
      "for i, label in enumerate(set(test_cats)):\n",
      " topN = np.argsort(bayes_classifier.coef_[i])[-N:]\n",
      " print \"\\nThe top %d most informative features for topic code %s: \\n%s\" % (N, label, \" \".join(vocabulary[topN]))\n",
      " #print topN\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MODEL: Multinomial Naive Bayes\n",
        "The precision for this classifier is 0.458351488744\n",
        "The recall for this classifier is 0.433333333333\n",
        "The f1 for this classifier is 0.373966606618\n",
        "The accuracy for this classifier is 0.433333333333\n",
        "\n",
        "Here is the classification report:\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          1       0.30      0.95      0.46        65\n",
        "          2       0.86      0.57      0.68        53\n",
        "          3       0.76      0.38      0.51        34\n",
        "          4       0.86      0.32      0.46        38\n",
        "          5       0.00      0.00      0.00        31\n",
        "          6       0.00      0.00      0.00        23\n",
        "          7       0.00      0.00      0.00        26\n",
        "\n",
        "avg / total       0.46      0.43      0.37       270\n",
        "\n",
        "\n",
        "Here is the confusion matrix:\n",
        "[[62  1  0  2  0  0  0]\n",
        " [23 30  0  0  0  0  0]\n",
        " [20  1 13  0  0  0  0]\n",
        " [26  0  0 12  0  0  0]\n",
        " [29  1  1  0  0  0  0]\n",
        " [22  1  0  0  0  0  0]\n",
        " [22  1  3  0  0  0  0]]\n",
        "\n",
        "The top 10 most informative features for topic code 1: \n",
        "job xa money like want does people know yahoo best\n",
        "\n",
        "The top 10 most informative features for topic code 3: \n",
        "does software windows internet web best use xa computer yahoo\n",
        "\n",
        "The top 10 most informative features for topic code 2: \n",
        "new think tv best music did favorite movie song xa\n",
        "\n",
        "The top 10 most informative features for topic code 5: \n",
        "know want sex women boyfriend guy friend girl like love\n",
        "\n",
        "The top 10 most informative features for topic code 4: \n",
        "study come world need help language college word does school\n",
        "\n",
        "The top 10 most informative features for topic code 7: \n",
        "surgery bad rid know cold fight pain way does best\n",
        "\n",
        "The top 10 most informative features for topic code 6: \n",
        "planet theory gas life possible earth moon xa world does\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/fayeip/anaconda/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1734: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels ['5' '6' '7']. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels ['5' '6' '7']. \n",
        "  average=average)\n",
        "/Users/fayeip/anaconda/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1809: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels ['5' '6' '7']. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels ['5' '6' '7']. \n",
        "  average=average)\n",
        "/Users/fayeip/anaconda/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1249: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels ['5' '6' '7']. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels ['5' '6' '7']. \n",
        "  average=average)\n",
        "/Users/fayeip/anaconda/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels ['5' '6' '7']. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels ['5' '6' '7']. \n",
        "  average=None)\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##now try another classifier\n",
      "\n",
      "svm_classifier = LinearSVC().fit(X_train_words, train_cats)\n",
      "\n",
      "yahoo_svm_predicted = svm_classifier.predict(X_test_words)\n",
      "print \"MODEL: Linear SVC\"\n",
      "\n",
      "print 'The precision for this classifier is ' + str(metrics.precision_score(test_cats, yahoo_svm_predicted))\n",
      "print 'The recall for this classifier is ' + str(metrics.recall_score(test_cats, yahoo_svm_predicted))\n",
      "print 'The f1 for this classifier is ' + str(metrics.f1_score(test_cats, yahoo_svm_predicted))\n",
      "print 'The accuracy for this classifier is ' + str(metrics.accuracy_score(test_cats, yahoo_svm_predicted))\n",
      "\n",
      "print '\\nHere is the classification report:'\n",
      "print classification_report(test_cats, yahoo_svm_predicted)\n",
      "\n",
      "#simple thing to do would be to up the n-grams to bigrams; try varying ngram_range from (1, 1) to (1, 2)\n",
      "#we could also modify the vectorizer to stem or lemmatize\n",
      "print '\\nHere is the confusion matrix:'\n",
      "print metrics.confusion_matrix(test_cats, yahoo_svm_predicted)\n",
      "\n",
      "N = 10\n",
      "vocabulary = np.array([t for t, i in sorted(vectorizer.vocabulary_.iteritems(), key=itemgetter(1))])\n",
      "\n",
      "for i, label in enumerate(set(test_cats)):\n",
      " topN = np.argsort(svm_classifier.coef_[i])[-N:]\n",
      " print \"\\nThe top %d most informative features for topic code %s: \\n%s\" % (N, label, \" \".join(vocabulary[topN]))\n",
      " #print topN\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MODEL: Linear SVC\n",
        "The precision for this classifier is 0.637674780282\n",
        "The recall for this classifier is 0.592592592593\n",
        "The f1 for this classifier is 0.588089721416\n",
        "The accuracy for this classifier is 0.592592592593\n",
        "\n",
        "Here is the classification report:\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          1       0.44      0.74      0.55        65\n",
        "          2       0.83      0.75      0.79        53\n",
        "          3       0.71      0.65      0.68        34\n",
        "          4       0.62      0.61      0.61        38\n",
        "          5       0.55      0.39      0.45        31\n",
        "          6       0.88      0.30      0.45        23\n",
        "          7       0.57      0.31      0.40        26\n",
        "\n",
        "avg / total       0.64      0.59      0.59       270\n",
        "\n",
        "\n",
        "Here is the confusion matrix:\n",
        "[[48  3  1  7  3  1  2]\n",
        " [11 40  1  0  0  0  1]\n",
        " [ 8  0 22  3  1  0  0]\n",
        " [13  0  1 23  0  0  1]\n",
        " [11  2  2  2 12  0  2]\n",
        " [ 9  1  1  2  3  7  0]\n",
        " [10  2  3  0  3  0  8]]\n",
        "\n",
        "The top 10 most informative features for topic code 1: \n",
        "santa addicted growth job jesus investment money stock credit phone\n",
        "\n",
        "The top 10 most informative features for topic code 3: \n",
        "programming file rss linux java laptop use windows internet computer\n",
        "\n",
        "The top 10 most informative features for topic code 2: \n",
        "actress movies film singer tv magazine rock movie music song\n",
        "\n",
        "The top 10 most informative features for topic code 5: \n",
        "girls husband married family dating date relationship friend love marriage\n",
        "\n",
        "The top 10 most informative features for topic code 4: \n",
        "largest sudoku word shot college university colleges study education school\n",
        "\n",
        "The top 10 most informative features for topic code 7: \n",
        "fart hiv allergic pain weight teeth fight aids diet surgery\n",
        "\n",
        "The top 10 most informative features for topic code 6: \n",
        "nipples humans sky stars universe moon planet eyes math theory\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "###now try max ent as a classifier\n",
      "maxent_classifier = LogisticRegression().fit(X_train_words, train_cats)\n",
      "\n",
      "yahoo_maxent_predicted = maxent_classifier.predict(X_test_words)\n",
      "print \"MODEL: Maximum Entropy\"\n",
      "\n",
      "print 'The precision for this classifier is ' + str(metrics.precision_score(test_cats, yahoo_maxent_predicted))\n",
      "print 'The recall for this classifier is ' + str(metrics.recall_score(test_cats, yahoo_maxent_predicted))\n",
      "print 'The f1 for this classifier is ' + str(metrics.f1_score(test_cats, yahoo_maxent_predicted))\n",
      "print 'The accuracy for this classifier is ' + str(metrics.accuracy_score(test_cats, yahoo_maxent_predicted))\n",
      "\n",
      "print '\\nHere is the classification report:'\n",
      "print classification_report(test_cats, yahoo_maxent_predicted)\n",
      "\n",
      "#simple thing to do would be to up the n-grams to bigrams; try varying ngram_range from (1, 1) to (1, 2)\n",
      "#we could also modify the vectorizer to stem or lemmatize\n",
      "print '\\nHere is the confusion matrix:'\n",
      "print metrics.confusion_matrix(test_cats, yahoo_maxent_predicted)\n",
      "\n",
      "N = 10\n",
      "vocabulary = np.array([t for t, i in sorted(vectorizer.vocabulary_.iteritems(), key=itemgetter(1))])\n",
      "\n",
      "for i, label in enumerate(set(test_cats)):\n",
      " topN = np.argsort(maxent_classifier.coef_[i])[-N:]\n",
      " print \"\\nThe top %d most informative features for topic code %s: \\n%s\" % (N, label, \" \".join(vocabulary[topN]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MODEL: Maximum Entropy\n",
        "The precision for this classifier is 0.613452380952\n",
        "The recall for this classifier is 0.477777777778\n",
        "The f1 for this classifier is 0.433959668368\n",
        "The accuracy for this classifier is 0.477777777778\n",
        "\n",
        "Here is the classification report:\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          1       0.31      0.88      0.46        65\n",
        "          2       0.88      0.66      0.75        53\n",
        "          3       0.85      0.50      0.63        34\n",
        "          4       0.75      0.47      0.58        38\n",
        "          5       0.50      0.03      0.06        31\n",
        "          6       0.00      0.00      0.00        23\n",
        "          7       1.00      0.04      0.07        26\n",
        "\n",
        "avg / total       0.61      0.48      0.43       270\n",
        "\n",
        "\n",
        "Here is the confusion matrix:\n",
        "[[57  2  0  4  1  1  0]\n",
        " [18 35  0  0  0  0  0]\n",
        " [16  0 17  1  0  0  0]\n",
        " [20  0  0 18  0  0  0]\n",
        " [28  1  1  0  1  0  0]\n",
        " [21  1  0  1  0  0  0]\n",
        " [22  1  2  0  0  0  1]]\n",
        "\n",
        "The top 10 most informative features for topic code 1: \n",
        "california question house people business stock phone credit job money\n",
        "\n",
        "The top 10 most informative features for topic code 3: \n",
        "laptop page linux web software yahoo internet windows use computer\n",
        "\n",
        "The top 10 most informative features for topic code 2: \n",
        "episode did movies magazine tv rock favorite movie music song\n",
        "\n",
        "The top 10 most informative features for topic code 5: \n",
        "girls date marriage girl guy women boyfriend relationship friend love\n",
        "\n",
        "The top 10 most informative features for topic code 4: \n",
        "book colleges university high language education study word college school\n",
        "\n",
        "The top 10 most informative features for topic code 7: \n",
        "hiv smoking causes blood aids rid diet fight surgery pain\n",
        "\n",
        "The top 10 most informative features for topic code 6: \n",
        "possible stars eyes universe math planet gas earth moon theory\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/fayeip/anaconda/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1734: UserWarning: The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels ['6']. \n",
        "  average=average)\n",
        "/Users/fayeip/anaconda/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1809: UserWarning: The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels ['6']. \n",
        "  average=average)\n",
        "/Users/fayeip/anaconda/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1249: UserWarning: The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels ['6']. \n",
        "  average=average)\n",
        "/Users/fayeip/anaconda/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels ['6']. \n",
        "  average=None)\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#function to throw out the most occuring and least occuring words;\n",
      "#do this by term frequency for a word, and then figuring out the overall tf distribution \n",
      "#of words in the collection\n",
      "def get_ngram_cutoff(ngram, lowerbound, upperbound):\n",
      "  fdist_ngram = nltk.FreqDist(ngram)\n",
      "  fdist_keys = fdist_ngram.keys()\n",
      "  fdist_vals =  fdist_ngram.values()\n",
      "  percentilescore = [stats.percentileofscore(fdist_vals, i) for i in fdist_vals]\n",
      "  new_fdist = dict(zip(fdist_keys, percentilescore))\n",
      "  #get the keywords:\n",
      "  keywords = []  \n",
      "  new_key_word_dict = []\n",
      "  #if the term freq of a word is between the xth and yth percentile, keep it; if not, throw it out. \n",
      "  for word, percentile in new_fdist.iteritems():\n",
      "      if percentile > lowerbound and percentile < upperbound:\n",
      "         #get the words freq count\n",
      "        #new_key_word_dict[word[0]] = fdist_ngram[word]\n",
      "        new_key_word_dict.append( (word) + (fdist_ngram[word],))\n",
      "  sorted_keywords = sorted_key_word_dict = sorted(new_key_word_dict,  key=lambda tup: tup[2], reverse = True)\n",
      "  return sorted_keywords "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 244
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##might be worth it to run a spell checker:\n",
      "import re, collections\n",
      "\n",
      "def words(text): return re.findall('[a-z]+', text.lower()) \n",
      "\n",
      "def train(features):\n",
      "    model = collections.defaultdict(lambda: 1)\n",
      "    for f in features:\n",
      "        model[f] += 1\n",
      "    return model\n",
      "\n",
      "NWORDS = train(words(file('big.txt').read()))\n",
      "\n",
      "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
      "\n",
      "def edits1(word):\n",
      "   splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
      "   deletes    = [a + b[1:] for a, b in splits if b]\n",
      "   transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]\n",
      "   replaces   = [a + c + b[1:] for a, b in splits for c in alphabet if b]\n",
      "   inserts    = [a + c + b     for a, b in splits for c in alphabet]\n",
      "   return set(deletes + transposes + replaces + inserts)\n",
      "\n",
      "def known_edits2(word):\n",
      "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in NWORDS)\n",
      "\n",
      "def known(words): return set(w for w in words if w in NWORDS)\n",
      "\n",
      "def correct(word):\n",
      "    candidates = known([word]) or known(edits1(word)) or known_edits2(word) or [word]\n",
      "    return max(candidates, key=NWORDS.get)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IOError",
       "evalue": "[Errno 2] No such file or directory: 'big.txt'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-20-107252f51f78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mNWORDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'big.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0malphabet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'abcdefghijklmnopqrstuvwxyz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'big.txt'"
       ]
      }
     ],
     "prompt_number": 20
    }
   ],
   "metadata": {}
  }
 ]
}